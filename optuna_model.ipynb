{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time\n",
    "import shap\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwangju_preprocessed = pd.read_csv(\"Gwangju_preprocess.csv\")\n",
    "busan_preprocessed = pd.read_csv(\"Busan_preprocess.csv\")\n",
    "daegu_preprocessed = pd.read_csv(\"Daegu_preprocess.csv\")\n",
    "daejeon_preprocessed = pd.read_csv(\"Daejeon_preprocess.csv\")\n",
    "incheon_preprocessed = pd.read_csv(\"Incheon_preprocess.csv\")\n",
    "seoul_preprocessed = pd.read_csv(\"Seoul_preprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>Humi</th>\n",
       "      <th>WS</th>\n",
       "      <th>WD</th>\n",
       "      <th>Solar</th>\n",
       "      <th>Day_sin</th>\n",
       "      <th>Day_cos</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>...</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "      <th>D1_Temp</th>\n",
       "      <th>D1_Humi</th>\n",
       "      <th>D1_WS</th>\n",
       "      <th>D1_Solar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.8</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.7</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40178</th>\n",
       "      <td>-2.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40179</th>\n",
       "      <td>-1.9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40180</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40181</th>\n",
       "      <td>-2.4</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40182</th>\n",
       "      <td>-2.1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40183 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temp  Humi   WS     WD  Solar   Day_sin   Day_cos  T8  T9  T10  ...  \\\n",
       "0      -7.8  83.0  0.0    0.0   0.00  0.017213  0.999852   1   0    0  ...   \n",
       "1      -5.7  74.0  0.0    0.0   0.22  0.017213  0.999852   0   1    0  ...   \n",
       "2      -3.2  61.0  3.5   50.0   1.13  0.017213  0.999852   0   0    1  ...   \n",
       "3      -2.7  57.0  1.2  360.0   1.70  0.017213  0.999852   0   0    0  ...   \n",
       "4      -1.4  55.0  1.6   50.0   2.00  0.017213  0.999852   0   0    0  ...   \n",
       "...     ...   ...  ...    ...    ...       ...       ...  ..  ..  ...  ...   \n",
       "40178  -2.5  76.0  1.4  230.0   1.36  0.017213  0.999852   0   0    0  ...   \n",
       "40179  -1.9  78.0  1.4  230.0   1.29  0.017213  0.999852   0   0    0  ...   \n",
       "40180  -2.3  86.0  0.5  200.0   0.70  0.017213  0.999852   0   0    0  ...   \n",
       "40181  -2.4  94.0  1.7  230.0   0.26  0.017213  0.999852   0   0    0  ...   \n",
       "40182  -2.1  96.0  1.5  200.0   0.08  0.017213  0.999852   0   0    0  ...   \n",
       "\n",
       "       T13  T14  T15  T16  T17  T18  D1_Temp  D1_Humi  D1_WS  D1_Solar  \n",
       "0        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "1        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "2        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "3        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "4        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "...    ...  ...  ...  ...  ...  ...      ...      ...    ...       ...  \n",
       "40178    0    1    0    0    0    0     -5.0     85.0    1.5      0.34  \n",
       "40179    0    0    1    0    0    0     -5.0     86.0    1.4      0.21  \n",
       "40180    0    0    0    1    0    0     -5.3     83.0    2.3      0.18  \n",
       "40181    0    0    0    0    1    0     -5.8     91.0    1.8      0.11  \n",
       "40182    0    0    0    0    0    1     -6.2     93.0    1.0      0.06  \n",
       "\n",
       "[40183 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gwangju_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwangju_train = gwangju_preprocessed.iloc[:24112, :]\n",
    "gwangju_val = gwangju_preprocessed.iloc[24112:32142, :]\n",
    "gwangju_test = gwangju_preprocessed.iloc[32142:, :]\n",
    "gwangju_train_val = gwangju_preprocessed.iloc[:32142, :]\n",
    "\n",
    "busan_train = busan_preprocessed.iloc[:24112, :]\n",
    "busan_val = busan_preprocessed.iloc[24112:32142, :]\n",
    "busan_test = busan_preprocessed.iloc[32142:, :]\n",
    "busan_train_val = busan_preprocessed.iloc[:32142, :]\n",
    "\n",
    "daegu_train = daegu_preprocessed.iloc[:24112, :]\n",
    "daegu_val = daegu_preprocessed.iloc[24112:32142, :]\n",
    "daegu_test = daegu_preprocessed.iloc[32142:, :]\n",
    "daegu_train_val = daegu_preprocessed.iloc[:32142, :]\n",
    "\n",
    "daejeon_train = daejeon_preprocessed.iloc[:24112, :]\n",
    "daejeon_val = daejeon_preprocessed.iloc[24112:32142, :]\n",
    "daejeon_test = daejeon_preprocessed.iloc[32142:, :]\n",
    "daejeon_train_val = daejeon_preprocessed.iloc[:32142, :]\n",
    "\n",
    "incheon_train = incheon_preprocessed.iloc[:24112, :]\n",
    "incheon_val = incheon_preprocessed.iloc[24112:32142, :]\n",
    "incheon_test = incheon_preprocessed.iloc[32142:, :]\n",
    "incheon_train_val = incheon_preprocessed.iloc[:32142, :]\n",
    "\n",
    "seoul_train = seoul_preprocessed.iloc[:24112, :]\n",
    "seoul_val = seoul_preprocessed.iloc[24112:32142, :]\n",
    "seoul_test = seoul_preprocessed.iloc[32142:, :]\n",
    "seoul_train_val = seoul_preprocessed.iloc[:32142, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.모델 입력에 맞게 데이터 변환   (코드 실행시 각 지역을 한작업으로 수행합니다. 총 6작업으로 나누어 진행하였습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지역 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gwangju\n",
    "X_train_rf_gbm = gwangju_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = gwangju_train['Solar']\n",
    "X_val_rf_gbm = gwangju_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = gwangju_val['Solar']\n",
    "X_test_rf_gbm = gwangju_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = gwangju_test['Solar']\n",
    "X_train_val_rf_gbm = gwangju_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = gwangju_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busan\n",
    "X_train_rf_gbm = busan_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = busan_train['Solar']\n",
    "X_val_rf_gbm = busan_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = busan_val['Solar']\n",
    "X_test_rf_gbm = busan_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = busan_test['Solar']\n",
    "X_train_val_rf_gbm = busan_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = busan_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daegu\n",
    "X_train_rf_gbm = daegu_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = daegu_train['Solar']\n",
    "X_val_rf_gbm = daegu_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = daegu_val['Solar']\n",
    "X_test_rf_gbm = daegu_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = daegu_test['Solar']\n",
    "X_train_val_rf_gbm = daegu_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = daegu_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daejeon\n",
    "X_train_rf_gbm = daejeon_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = daejeon_train['Solar']\n",
    "X_val_rf_gbm = daejeon_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = daejeon_val['Solar']\n",
    "X_test_rf_gbm = daejeon_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = daejeon_test['Solar']\n",
    "X_train_val_rf_gbm = daejeon_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = daejeon_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incheon\n",
    "X_train_rf_gbm = incheon_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = incheon_train['Solar']\n",
    "X_val_rf_gbm = incheon_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = incheon_val['Solar']\n",
    "X_test_rf_gbm = incheon_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = incheon_test['Solar']\n",
    "X_train_val_rf_gbm = incheon_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = incheon_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seoul\n",
    "X_train_rf_gbm = seoul_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = seoul_train['Solar']\n",
    "X_val_rf_gbm = seoul_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = seoul_val['Solar']\n",
    "X_test_rf_gbm = seoul_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = seoul_test['Solar']\n",
    "X_train_val_rf_gbm = seoul_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = seoul_train_val['Solar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 지역 공통"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost를 위한 데이터 준비 공통\n",
    "import xgboost as xgb\n",
    "\n",
    "# 범주형 변수 설정\n",
    "dtrain = xgb.DMatrix(X_train_rf_gbm, label=y_train_rf_gbm)\n",
    "dval = xgb.DMatrix(X_val_rf_gbm, label=y_val_rf_gbm)\n",
    "dtest = xgb.DMatrix(X_test_rf_gbm, label=y_test_rf_gbm)\n",
    "dtrain_val = xgb.DMatrix(X_train_val_rf_gbm, label=y_train_val_rf_gbm)\n",
    "\n",
    "# LightGBM을 위한 데이터 준비 공통\n",
    "import lightgbm as lgb\n",
    "\n",
    "ltrain = lgb.Dataset(X_train_rf_gbm, label=y_train_rf_gbm)\n",
    "lval = lgb.Dataset(X_val_rf_gbm, label=y_val_rf_gbm)\n",
    "ltest = lgb.Dataset(X_test_rf_gbm, label=y_test_rf_gbm)\n",
    "ltrain_val = lgb.Dataset(X_train_val_rf_gbm, label=y_train_val_rf_gbm)\n",
    "\n",
    "# CatBoost를 위한 데이터 준비 공통\n",
    "import catboost as cb\n",
    "\n",
    "ctrain = cb.Pool(data=X_train_rf_gbm, label=y_train_rf_gbm)\n",
    "cval = cb.Pool(data=X_val_rf_gbm, label=y_val_rf_gbm)\n",
    "ctest = cb.Pool(data=X_test_rf_gbm, label=y_test_rf_gbm)\n",
    "ctrain_val = cb.Pool(data=X_train_val_rf_gbm, label=y_train_val_rf_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "     -------------------------------------- 364.4/364.4 KB 2.8 MB/s eta 0:00:00\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\양태영\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Collecting sqlalchemy>=1.4.2\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.5/233.5 KB 2.4 MB/s eta 0:00:00\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     -------------------------------------- 161.8/161.8 KB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\양태영\\appdata\\roaming\\python\\python310\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\양태영\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\양태영\\appdata\\roaming\\python\\python310\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "     -------------------------------------- 298.4/298.4 KB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\양태영\\appdata\\roaming\\python\\python310\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Installing collected packages: PyYAML, MarkupSafe, greenlet, colorlog, sqlalchemy, Mako, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 MarkupSafe-3.0.2 PyYAML-6.0.2 alembic-1.14.0 colorlog-6.9.0 greenlet-3.1.1 optuna-4.1.0 sqlalchemy-2.0.36\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script mako-render.exe is installed in 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script optuna.exe is installed in 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna + seed 고정 / train으로 학습 val을 통해 평가\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42  # 랜덤 시드 설정 추가\n",
    "\n",
    "# Random Forest Objective\n",
    "def objective_rf(trial):\n",
    "    \n",
    "    np.random.seed(SEED)  # 랜덤 시드 설정 추가\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [128]),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, random_state=SEED)\n",
    "    model.fit(X_train_rf_gbm, y_train_rf_gbm)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# Gradient Boosting Objective\n",
    "def objective_gbm(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 250, 500]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5, 10]),\n",
    "        'loss': trial.suggest_categorical('loss', ['quantile', 'huber'])\n",
    "    }\n",
    "    model = GradientBoostingRegressor(**params, random_state=SEED)\n",
    "    model.fit(X_train_rf_gbm, y_train_rf_gbm)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# XGBoost Objective for Optuna\n",
    "def objective_xgb(trial):\n",
    "    # Hyperparameter suggestion\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [6, 8, 10]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5, 0.75, 1.0]),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.75, 1.0]),\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'random_state': SEED  # XGBoost의 랜덤 시드 설정 추가\n",
    "    }\n",
    "    # Separate num_boost_round from params\n",
    "    num_boost_round = trial.suggest_categorical('n_estimators', [250, 500, 1000])\n",
    "\n",
    "    # XGBoost Training with DMatrix\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = model.predict(dval)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(dval.get_label(), predictions))\n",
    "    return rmse\n",
    "\n",
    "# LightGBM Objective\n",
    "def objective_lgb(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 64, 64),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5]),\n",
    "        'feature_fraction': trial.suggest_categorical('feature_fraction', [1.0]),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "        'seed': SEED  # LightGBM의 랜덤 시드 설정 추가\n",
    "    }\n",
    "    num_boost_round = trial.suggest_categorical('n_estimators', [1000, 1500])\n",
    "    model = lgb.train(params, ltrain, num_boost_round=num_boost_round)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# CatBoost Objective\n",
    "def objective_cat(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.1]),\n",
    "        'depth': trial.suggest_categorical('depth', [4, 6, 10]),\n",
    "        'l2_leaf_reg': trial.suggest_categorical('l2_leaf_reg', [1, 3, 5, 7, 9]),\n",
    "        'random_state': SEED\n",
    "    }\n",
    "    model = cb.CatBoostRegressor(**params,verbose=0)\n",
    "    model.fit(ctrain)\n",
    "    predictions = model.predict(cval)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Optuna 하이퍼파라미터 최적화(train : val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna + seed 고정 / train으로 학습 val을 통해 평가\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42  # 랜덤 시드 설정 추가\n",
    "\n",
    "# Random Forest Objective\n",
    "def objective_rf(trial):\n",
    "    \n",
    "    np.random.seed(SEED)  # 랜덤 시드 설정 추가\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [128]),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, random_state=SEED)\n",
    "    model.fit(X_train_rf_gbm, y_train_rf_gbm)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# Gradient Boosting Objective\n",
    "def objective_gbm(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 250, 500]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5, 10]),\n",
    "        'loss': trial.suggest_categorical('loss', ['quantile', 'huber'])\n",
    "    }\n",
    "    model = GradientBoostingRegressor(**params, random_state=SEED)\n",
    "    model.fit(X_train_rf_gbm, y_train_rf_gbm)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# XGBoost Objective for Optuna\n",
    "def objective_xgb(trial):\n",
    "    # Hyperparameter suggestion\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [6, 8, 10]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5, 0.75, 1.0]),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.75, 1.0]),\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'random_state': SEED  # XGBoost의 랜덤 시드 설정 추가\n",
    "    }\n",
    "    # Separate num_boost_round from params\n",
    "    num_boost_round = trial.suggest_categorical('n_estimators', [250, 500, 1000])\n",
    "\n",
    "    # XGBoost Training with DMatrix\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = model.predict(dval)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(dval.get_label(), predictions))\n",
    "    return rmse\n",
    "\n",
    "# LightGBM Objective\n",
    "def objective_lgb(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 64, 64),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5]),\n",
    "        'feature_fraction': trial.suggest_categorical('feature_fraction', [1.0]),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "        'seed': SEED  # LightGBM의 랜덤 시드 설정 추가\n",
    "    }\n",
    "    num_boost_round = trial.suggest_categorical('n_estimators', [1000, 1500])\n",
    "    model = lgb.train(params, ltrain, num_boost_round=num_boost_round)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# CatBoost Objective\n",
    "def objective_cat(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.03, 0.1]),\n",
    "        'depth': trial.suggest_categorical('depth', [4, 6, 10]),\n",
    "        'l2_leaf_reg': trial.suggest_categorical('l2_leaf_reg', [1, 3, 5, 7, 9]),\n",
    "        'random_state': SEED\n",
    "    }\n",
    "    model = cb.CatBoostRegressor(**params,verbose=0)\n",
    "    model.fit(ctrain)\n",
    "    predictions = model.predict(cval)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 16:48:10,247] A new study created in memory with name: no-name-793010a5-7c61-4c47-8b92-c490ee6135e3\n",
      "[I 2024-12-02 16:48:22,499] Trial 0 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:48:34,312] Trial 1 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:48:45,825] Trial 2 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:48:58,011] Trial 3 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:49:09,520] Trial 4 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:49:21,818] Trial 5 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:49:34,353] Trial 6 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:49:45,656] Trial 7 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:49:55,916] Trial 8 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:50:06,507] Trial 9 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:50:17,000] Trial 10 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:50:28,260] Trial 11 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:50:39,114] Trial 12 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:50:50,214] Trial 13 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:51:00,934] Trial 14 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:51:11,783] Trial 15 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:51:23,706] Trial 16 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:51:35,332] Trial 17 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:51:45,868] Trial 18 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:51:56,228] Trial 19 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:52:06,247] Trial 20 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:52:17,827] Trial 21 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:52:27,754] Trial 22 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:52:37,574] Trial 23 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:52:47,859] Trial 24 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:52:59,116] Trial 25 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:53:09,851] Trial 26 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:53:19,202] Trial 27 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:53:28,662] Trial 28 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:53:38,296] Trial 29 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:53:47,906] Trial 30 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:53:57,379] Trial 31 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:54:06,433] Trial 32 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:54:15,632] Trial 33 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:54:25,201] Trial 34 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:54:35,378] Trial 35 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:54:44,952] Trial 36 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:54:54,024] Trial 37 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:55:03,286] Trial 38 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:55:12,933] Trial 39 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:55:22,640] Trial 40 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:55:32,256] Trial 41 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:55:41,498] Trial 42 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:55:50,595] Trial 43 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:00,247] Trial 44 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:09,956] Trial 45 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:19,405] Trial 46 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:28,823] Trial 47 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:38,023] Trial 48 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:47,253] Trial 49 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:56:56,810] Trial 50 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:57:06,468] Trial 51 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:57:17,715] Trial 52 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:57:28,340] Trial 53 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:57:38,552] Trial 54 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:57:48,107] Trial 55 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:57:57,582] Trial 56 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:58:06,960] Trial 57 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:58:16,406] Trial 58 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:58:25,804] Trial 59 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:58:35,322] Trial 60 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:58:44,933] Trial 61 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:58:54,160] Trial 62 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:03,319] Trial 63 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:12,471] Trial 64 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:21,901] Trial 65 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:31,620] Trial 66 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:41,364] Trial 67 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:50,782] Trial 68 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 16:59:59,991] Trial 69 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:00:09,376] Trial 70 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:00:18,941] Trial 71 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:00:28,680] Trial 72 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:00:38,007] Trial 73 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:00:47,188] Trial 74 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:00:56,450] Trial 75 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:01:05,977] Trial 76 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:01:15,882] Trial 77 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:01:25,478] Trial 78 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:01:34,643] Trial 79 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:01:43,889] Trial 80 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:01:53,222] Trial 81 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:02,832] Trial 82 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:12,317] Trial 83 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:21,922] Trial 84 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:31,100] Trial 85 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:40,503] Trial 86 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:50,068] Trial 87 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:02:59,763] Trial 88 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:03:09,506] Trial 89 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:03:18,852] Trial 90 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:03:28,114] Trial 91 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:03:37,413] Trial 92 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:03:47,188] Trial 93 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:03:56,825] Trial 94 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:04:06,255] Trial 95 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:04:15,617] Trial 96 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:04:24,718] Trial 97 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:04:34,305] Trial 98 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-02 17:04:43,871] Trial 99 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n"
     ]
    }
   ],
   "source": [
    "study_rf = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_rf.optimize(objective_rf, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 17:04:43,926] A new study created in memory with name: no-name-73d6e8f3-86cc-44d2-971b-6be6bd80afb4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 17:06:34,429] Trial 0 finished with value: 0.39914551467349274 and parameters: {'n_estimators': 250, 'learning_rate': 0.01, 'max_depth': 10, 'loss': 'huber'}. Best is trial 0 with value: 0.39914551467349274.\n",
      "[I 2024-12-02 17:07:39,567] Trial 1 finished with value: 0.657040855725531 and parameters: {'n_estimators': 250, 'learning_rate': 0.01, 'max_depth': 10, 'loss': 'quantile'}. Best is trial 0 with value: 0.39914551467349274.\n",
      "[I 2024-12-02 17:08:01,995] Trial 2 finished with value: 0.4672809411911528 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 10, 'loss': 'quantile'}. Best is trial 0 with value: 0.39914551467349274.\n",
      "[I 2024-12-02 17:08:11,531] Trial 3 finished with value: 0.5703221532675726 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 0 with value: 0.39914551467349274.\n",
      "[I 2024-12-02 17:09:18,168] Trial 4 finished with value: 0.657040855725531 and parameters: {'n_estimators': 250, 'learning_rate': 0.01, 'max_depth': 10, 'loss': 'quantile'}. Best is trial 0 with value: 0.39914551467349274.\n",
      "[I 2024-12-02 17:09:53,135] Trial 5 finished with value: 0.3787252819019376 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 10, 'loss': 'huber'}. Best is trial 5 with value: 0.3787252819019376.\n",
      "[I 2024-12-02 17:12:28,402] Trial 6 finished with value: 0.38599493171279686 and parameters: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 10, 'loss': 'huber'}. Best is trial 5 with value: 0.3787252819019376.\n",
      "[I 2024-12-02 17:12:38,012] Trial 7 finished with value: 1.0746769230883073 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 5 with value: 0.3787252819019376.\n",
      "[I 2024-12-02 17:12:47,517] Trial 8 finished with value: 0.5066957197763132 and parameters: {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 5 with value: 0.3787252819019376.\n",
      "[I 2024-12-02 17:13:37,323] Trial 9 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:14:26,786] Trial 10 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:15:17,065] Trial 11 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:16:07,096] Trial 12 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:16:57,030] Trial 13 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:17:46,930] Trial 14 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:18:37,141] Trial 15 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:19:27,440] Trial 16 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:20:17,405] Trial 17 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:21:07,211] Trial 18 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:21:56,809] Trial 19 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:22:21,973] Trial 20 finished with value: 0.36822815887380284 and parameters: {'n_estimators': 250, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:23:12,342] Trial 21 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:24:02,478] Trial 22 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:24:51,877] Trial 23 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:25:41,752] Trial 24 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:26:32,341] Trial 25 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:27:23,097] Trial 26 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:28:12,438] Trial 27 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:28:38,771] Trial 28 finished with value: 0.3664258149523064 and parameters: {'n_estimators': 250, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:32:04,661] Trial 29 finished with value: 0.377537576838528 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:32:29,490] Trial 30 finished with value: 0.4391073519801904 and parameters: {'n_estimators': 250, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:33:18,236] Trial 31 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:34:07,483] Trial 32 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:34:56,646] Trial 33 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:36:34,753] Trial 34 finished with value: 0.4651014625330975 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:36:43,867] Trial 35 finished with value: 0.5703221532675726 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:38:34,213] Trial 36 finished with value: 0.39914551467349274 and parameters: {'n_estimators': 250, 'learning_rate': 0.01, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:39:25,697] Trial 37 finished with value: 0.37201770858093003 and parameters: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:39:49,848] Trial 38 finished with value: 0.49370947854932046 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:40:38,683] Trial 39 finished with value: 0.37201770858093003 and parameters: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:41:25,150] Trial 40 finished with value: 0.5719651360617369 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:42:16,896] Trial 41 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:43:06,475] Trial 42 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:43:55,494] Trial 43 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:44:05,249] Trial 44 finished with value: 0.3909636572561433 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:44:54,346] Trial 45 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:47:31,014] Trial 46 finished with value: 0.37974979571609385 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:48:20,363] Trial 47 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:48:43,178] Trial 48 finished with value: 0.4846189094814341 and parameters: {'n_estimators': 250, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:49:32,201] Trial 49 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:49:41,912] Trial 50 finished with value: 0.3909636572561433 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:50:31,428] Trial 51 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:51:20,922] Trial 52 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:52:10,509] Trial 53 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:52:59,698] Trial 54 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:53:48,762] Trial 55 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:54:38,289] Trial 56 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:55:27,726] Trial 57 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:56:17,642] Trial 58 finished with value: 0.38994006105056395 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:57:40,805] Trial 59 finished with value: 0.3775199426187237 and parameters: {'n_estimators': 250, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:58:29,820] Trial 60 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 17:59:19,483] Trial 61 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:00:08,503] Trial 62 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:00:59,442] Trial 63 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:01:50,387] Trial 64 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:02:40,597] Trial 65 finished with value: 0.37201770858093003 and parameters: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:03:30,743] Trial 66 finished with value: 0.4868341985984668 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:04:20,454] Trial 67 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:04:30,664] Trial 68 finished with value: 0.5731386156067861 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:07:07,318] Trial 69 finished with value: 0.37974979571609385 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:07:31,957] Trial 70 finished with value: 0.36822815887380284 and parameters: {'n_estimators': 250, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:08:21,986] Trial 71 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:09:11,760] Trial 72 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:10:01,818] Trial 73 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:10:50,687] Trial 74 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:11:40,578] Trial 75 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:12:25,580] Trial 76 finished with value: 0.4797340144355913 and parameters: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:13:14,036] Trial 77 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:14:02,910] Trial 78 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:14:42,684] Trial 79 finished with value: 0.3786446167854145 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:15:32,995] Trial 80 finished with value: 0.38994006105056395 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:16:22,206] Trial 81 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:17:11,303] Trial 82 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:18:00,573] Trial 83 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:18:49,876] Trial 84 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:19:40,936] Trial 85 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:20:03,533] Trial 86 finished with value: 0.4982486760728443 and parameters: {'n_estimators': 250, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:20:53,061] Trial 87 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:21:42,506] Trial 88 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:22:30,583] Trial 89 finished with value: 0.37201770858093003 and parameters: {'n_estimators': 500, 'learning_rate': 0.1, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:23:20,402] Trial 90 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:24:09,920] Trial 91 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:24:59,073] Trial 92 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:25:48,311] Trial 93 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:26:37,458] Trial 94 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:29:13,382] Trial 95 finished with value: 0.37974979571609385 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:29:23,581] Trial 96 finished with value: 0.3909636572561433 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:30:15,579] Trial 97 finished with value: 0.38994006105056395 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:31:05,192] Trial 98 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 9 with value: 0.36626076814933495.\n",
      "[I 2024-12-02 18:31:49,543] Trial 99 finished with value: 0.4868341985984668 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 9 with value: 0.36626076814933495.\n"
     ]
    }
   ],
   "source": [
    "study_gbm = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_gbm.optimize(objective_gbm, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 18:31:49,604] A new study created in memory with name: no-name-158cf602-972d-4d6c-bb67-4716ccbea5e1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 18:31:54,674] Trial 0 finished with value: 0.37454453110694885 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 0 with value: 0.37454453110694885.\n",
      "[I 2024-12-02 18:40:15,855] Trial 1 finished with value: 0.3694452941417694 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 18:49:24,850] Trial 2 finished with value: 0.37398219108581543 and parameters: {'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.75, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 18:49:32,659] Trial 3 finished with value: 0.3787497282028198 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 18:49:37,105] Trial 4 finished with value: 0.37616512179374695 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'subsample': 1.0, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 18:49:41,328] Trial 5 finished with value: 0.3787100315093994 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 0.5, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 19:00:08,242] Trial 6 finished with value: 0.38295066356658936 and parameters: {'learning_rate': 0.1, 'max_depth': 10, 'subsample': 1.0, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 19:00:10,314] Trial 7 finished with value: 0.3786502778530121 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 500}. Best is trial 1 with value: 0.3694452941417694.\n",
      "[I 2024-12-02 19:00:45,709] Trial 8 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:00:52,901] Trial 9 finished with value: 0.38516202569007874 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 0.5, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:01:24,944] Trial 10 finished with value: 0.42666953802108765 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:02:00,030] Trial 11 finished with value: 0.41755756735801697 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:04:06,183] Trial 12 finished with value: 0.37731581926345825 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:04:37,771] Trial 13 finished with value: 0.4281696081161499 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:05:10,354] Trial 14 finished with value: 0.3741641938686371 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:07:43,887] Trial 15 finished with value: 0.3781042695045471 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'subsample': 1.0, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 500}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:08:17,051] Trial 16 finished with value: 0.41755756735801697 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:08:57,747] Trial 17 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:09:38,584] Trial 18 finished with value: 0.3775815963745117 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:10:15,335] Trial 19 finished with value: 0.37296611070632935 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:10:47,203] Trial 20 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:11:19,677] Trial 21 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:11:51,052] Trial 22 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:12:22,936] Trial 23 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:12:55,557] Trial 24 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:13:28,140] Trial 25 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:15:37,017] Trial 26 finished with value: 0.3816492557525635 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 500}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:16:17,341] Trial 27 finished with value: 0.3775815963745117 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:16:52,284] Trial 28 finished with value: 0.37296611070632935 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:16:53,321] Trial 29 finished with value: 0.3688211739063263 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:19:00,342] Trial 30 finished with value: 0.3734124004840851 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 500}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:19:31,807] Trial 31 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:20:03,474] Trial 32 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:20:34,281] Trial 33 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:21:05,251] Trial 34 finished with value: 0.3690314292907715 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:21:36,612] Trial 35 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:21:44,621] Trial 36 finished with value: 0.37979355454444885 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:22:16,360] Trial 37 finished with value: 0.3775815963745117 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:22:17,600] Trial 38 finished with value: 0.37046605348587036 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:32:23,795] Trial 39 finished with value: 0.3841398060321808 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:32:25,983] Trial 40 finished with value: 0.37654614448547363 and parameters: {'learning_rate': 0.1, 'max_depth': 8, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:32:57,199] Trial 41 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:33:29,012] Trial 42 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:34:01,601] Trial 43 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:42:11,386] Trial 44 finished with value: 0.37636029720306396 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:42:42,622] Trial 45 finished with value: 0.36856791377067566 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:44:46,632] Trial 46 finished with value: 0.39428937435150146 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 500}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:45:18,447] Trial 47 finished with value: 0.3700263798236847 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.75, 'booster': 'dart', 'n_estimators': 250}. Best is trial 8 with value: 0.36856791377067566.\n",
      "[I 2024-12-02 19:45:49,456] Trial 48 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:45:55,204] Trial 49 finished with value: 0.3955765664577484 and parameters: {'learning_rate': 0.01, 'max_depth': 10, 'subsample': 0.75, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:54:10,869] Trial 50 finished with value: 0.388817697763443 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:54:42,799] Trial 51 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:55:14,083] Trial 52 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:55:44,776] Trial 53 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:56:16,263] Trial 54 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:56:48,469] Trial 55 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:57:21,059] Trial 56 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 19:57:56,349] Trial 57 finished with value: 0.3706604540348053 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:00:01,953] Trial 58 finished with value: 0.3688717484474182 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:00:33,536] Trial 59 finished with value: 0.42005637288093567 and parameters: {'learning_rate': 0.01, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:01:14,107] Trial 60 finished with value: 0.3781338036060333 and parameters: {'learning_rate': 0.05, 'max_depth': 10, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:01:45,033] Trial 61 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:02:16,105] Trial 62 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:02:56,381] Trial 63 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:03:29,993] Trial 64 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:04:04,203] Trial 65 finished with value: 0.36809197068214417 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:04:38,491] Trial 66 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:05:12,462] Trial 67 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:05:47,137] Trial 68 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:05:49,400] Trial 69 finished with value: 0.38068893551826477 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 500}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:06:20,959] Trial 70 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:06:51,903] Trial 71 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:07:23,598] Trial 72 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:07:55,353] Trial 73 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:08:27,631] Trial 74 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:09:02,464] Trial 75 finished with value: 0.3696495592594147 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:09:34,672] Trial 76 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:17:56,833] Trial 77 finished with value: 0.3754766881465912 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:18:42,529] Trial 78 finished with value: 0.39401185512542725 and parameters: {'learning_rate': 0.01, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:19:17,002] Trial 79 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:19:49,008] Trial 80 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:20:20,411] Trial 81 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:20:51,668] Trial 82 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:21:23,264] Trial 83 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:21:54,560] Trial 84 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:21:56,035] Trial 85 finished with value: 0.367136150598526 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:22:27,939] Trial 86 finished with value: 0.3742421567440033 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:22:59,227] Trial 87 finished with value: 0.37218621373176575 and parameters: {'learning_rate': 0.1, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:25:16,773] Trial 88 finished with value: 0.37415432929992676 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:25:48,280] Trial 89 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:33:55,173] Trial 90 finished with value: 0.3754766881465912 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:34:27,157] Trial 91 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:34:58,621] Trial 92 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:35:29,953] Trial 93 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:36:01,420] Trial 94 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:36:32,570] Trial 95 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:37:05,129] Trial 96 finished with value: 0.36668699979782104 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:37:45,087] Trial 97 finished with value: 0.435498982667923 and parameters: {'learning_rate': 0.01, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:37:46,379] Trial 98 finished with value: 0.367136150598526 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 48 with value: 0.36668699979782104.\n",
      "[I 2024-12-02 20:38:18,475] Trial 99 finished with value: 0.36655282974243164 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 99 with value: 0.36655282974243164.\n"
     ]
    }
   ],
   "source": [
    "study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_xgb.optimize(objective_xgb, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:38:18,517] A new study created in memory with name: no-name-966136c1-2603-40af-8862-c14c6c677f9b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001333 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:38:23,705] Trial 0 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 0 with value: 0.376072387725667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:38:56,848] Trial 1 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 0 with value: 0.376072387725667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:39:04,616] Trial 2 finished with value: 0.3847183525046424 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 0 with value: 0.376072387725667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:39:55,192] Trial 3 finished with value: 0.3739486022983621 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 3 with value: 0.3739486022983621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:40:01,129] Trial 4 finished with value: 0.3847183525046424 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 3 with value: 0.3739486022983621.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:40:09,191] Trial 5 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 5 with value: 0.3672010780843957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:40:14,671] Trial 6 finished with value: 0.37879070648719537 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 5 with value: 0.3672010780843957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:40:20,767] Trial 7 finished with value: 0.37879070648719537 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 5 with value: 0.3672010780843957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:41:14,808] Trial 8 finished with value: 0.37544985926929475 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 5 with value: 0.3672010780843957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:08,805] Trial 9 finished with value: 0.3739486022983621 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 5 with value: 0.3672010780843957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:14,485] Trial 10 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:19,233] Trial 11 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:24,380] Trial 12 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:28,957] Trial 13 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:34,785] Trial 14 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:39,230] Trial 15 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:44,335] Trial 16 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:42:48,812] Trial 17 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:23,576] Trial 18 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:28,255] Trial 19 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:33,315] Trial 20 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:37,904] Trial 21 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:43,689] Trial 22 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:48,122] Trial 23 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:53,963] Trial 24 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002089 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:43:58,434] Trial 25 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:44:30,389] Trial 26 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:44:37,247] Trial 27 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:44:43,748] Trial 28 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:44:49,842] Trial 29 finished with value: 0.3826400719301149 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:44:55,170] Trial 30 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:44:59,989] Trial 31 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:45:05,776] Trial 32 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:45:10,420] Trial 33 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001850 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:45:45,078] Trial 34 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:45:48,670] Trial 35 finished with value: 0.3826400719301149 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:45:53,964] Trial 36 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:45:58,709] Trial 37 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:46:49,954] Trial 38 finished with value: 0.3739486022983621 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:46:55,133] Trial 39 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:47:01,419] Trial 40 finished with value: 0.37879070648719537 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:47:06,219] Trial 41 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:47:12,189] Trial 42 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:47:16,901] Trial 43 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:47:22,216] Trial 44 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:47:29,225] Trial 45 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:02,357] Trial 46 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:06,056] Trial 47 finished with value: 0.3826400719301149 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:11,333] Trial 48 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002497 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:18,096] Trial 49 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:23,578] Trial 50 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:28,254] Trial 51 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:33,603] Trial 52 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:38,198] Trial 53 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:43,611] Trial 54 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:48:48,342] Trial 55 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:22,720] Trial 56 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:28,261] Trial 57 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003507 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:33,436] Trial 58 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:37,371] Trial 59 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:43,837] Trial 60 finished with value: 0.3847183525046424 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:49,720] Trial 61 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:54,611] Trial 62 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:49:59,708] Trial 63 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:50:04,534] Trial 64 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:50:10,240] Trial 65 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:50:42,609] Trial 66 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:50:48,356] Trial 67 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:50:53,218] Trial 68 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:50:58,294] Trial 69 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:51:03,239] Trial 70 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:51:09,211] Trial 71 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:51:13,935] Trial 72 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:51:19,005] Trial 73 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:51:23,722] Trial 74 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:51:28,047] Trial 75 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:20,311] Trial 76 finished with value: 0.3739486022983621 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:25,165] Trial 77 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:30,387] Trial 78 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002153 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:35,224] Trial 79 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:40,545] Trial 80 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:45,279] Trial 81 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:50,416] Trial 82 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:52:55,107] Trial 83 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:00,306] Trial 84 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:05,348] Trial 85 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:10,566] Trial 86 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:16,873] Trial 87 finished with value: 0.37879070648719537 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:48,400] Trial 88 finished with value: 0.37394802062025345 and parameters: {'learning_rate': 0.1, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:54,014] Trial 89 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:53:59,399] Trial 90 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:04,353] Trial 91 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:09,344] Trial 92 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:14,127] Trial 93 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:19,157] Trial 94 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:24,217] Trial 95 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:29,204] Trial 96 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:34,030] Trial 97 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002342 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:39,165] Trial 98 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:46,036] Trial 99 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 10 with value: 0.366419897466168.\n"
     ]
    }
   ],
   "source": [
    "study_lgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_lgb.optimize(objective_lgb, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:46,074] A new study created in memory with name: no-name-540e960a-b2b6-4367-aeac-c081d9752fa4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 20:54:51,594] Trial 0 finished with value: 0.37397394515760507 and parameters: {'learning_rate': 0.1, 'depth': 4, 'l2_leaf_reg': 5}. Best is trial 0 with value: 0.37397394515760507.\n",
      "[I 2024-12-02 20:54:56,861] Trial 1 finished with value: 0.37397394515760507 and parameters: {'learning_rate': 0.1, 'depth': 4, 'l2_leaf_reg': 5}. Best is trial 0 with value: 0.37397394515760507.\n",
      "[I 2024-12-02 20:55:40,380] Trial 2 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:56:23,681] Trial 3 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:56:30,863] Trial 4 finished with value: 0.37397341895366776 and parameters: {'learning_rate': 0.1, 'depth': 6, 'l2_leaf_reg': 1}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:56:36,138] Trial 5 finished with value: 0.3765462594229397 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 1}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:56:41,057] Trial 6 finished with value: 0.37706570856220084 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:56:48,365] Trial 7 finished with value: 0.3683215547618619 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 3}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:56:53,729] Trial 8 finished with value: 0.3764153387932704 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 7}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:57:37,470] Trial 9 finished with value: 0.37732829462621015 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 2 with value: 0.36795421747982127.\n",
      "[I 2024-12-02 20:58:20,747] Trial 10 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 20:59:02,694] Trial 11 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 20:59:44,639] Trial 12 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:00:29,541] Trial 13 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:01:12,572] Trial 14 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:01:54,416] Trial 15 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:02:35,947] Trial 16 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:02:43,381] Trial 17 finished with value: 0.36873646806252364 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:03:25,192] Trial 18 finished with value: 0.37408273738375974 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:04:06,704] Trial 19 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:04:13,850] Trial 20 finished with value: 0.36867413364853174 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:04:55,849] Trial 21 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:05:37,415] Trial 22 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:06:18,973] Trial 23 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:07:00,535] Trial 24 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:07:42,119] Trial 25 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:08:24,490] Trial 26 finished with value: 0.37732829462621015 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:09:06,232] Trial 27 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:09:13,478] Trial 28 finished with value: 0.36873646806252364 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:09:55,857] Trial 29 finished with value: 0.3764734485088346 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:10:01,009] Trial 30 finished with value: 0.37706570856220084 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:10:42,965] Trial 31 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:11:25,155] Trial 32 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:12:07,000] Trial 33 finished with value: 0.36836910921410465 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:12:50,237] Trial 34 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:13:33,452] Trial 35 finished with value: 0.3769589333132876 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:14:14,963] Trial 36 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:14:20,208] Trial 37 finished with value: 0.3764153387932704 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:15:01,747] Trial 38 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:15:08,751] Trial 39 finished with value: 0.37301022484108615 and parameters: {'learning_rate': 0.1, 'depth': 6, 'l2_leaf_reg': 3}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:15:13,821] Trial 40 finished with value: 0.37706570856220084 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:15:55,355] Trial 41 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:16:37,017] Trial 42 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:17:19,166] Trial 43 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:18:00,941] Trial 44 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:18:43,307] Trial 45 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:19:27,662] Trial 46 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:19:32,634] Trial 47 finished with value: 0.3765801382266729 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 3}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:19:39,850] Trial 48 finished with value: 0.36867413364853174 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:20:24,107] Trial 49 finished with value: 0.37408273738375974 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:21:07,265] Trial 50 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:21:50,724] Trial 51 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:22:33,826] Trial 52 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:23:17,714] Trial 53 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:23:59,606] Trial 54 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:24:41,194] Trial 55 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:25:22,824] Trial 56 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:25:30,154] Trial 57 finished with value: 0.36873646806252364 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:26:11,893] Trial 58 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:26:53,691] Trial 59 finished with value: 0.377219063111122 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:27:39,943] Trial 60 finished with value: 0.36818732310370844 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:28:26,423] Trial 61 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:29:10,237] Trial 62 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:29:53,922] Trial 63 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:30:38,394] Trial 64 finished with value: 0.36836910921410465 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:31:23,402] Trial 65 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:31:28,507] Trial 66 finished with value: 0.37706570856220084 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:32:12,938] Trial 67 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:33:02,686] Trial 68 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:33:10,764] Trial 69 finished with value: 0.3725311809377064 and parameters: {'learning_rate': 0.1, 'depth': 6, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:33:56,841] Trial 70 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:34:41,838] Trial 71 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:35:26,554] Trial 72 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:36:10,045] Trial 73 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:36:53,939] Trial 74 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:37:38,112] Trial 75 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:38:22,451] Trial 76 finished with value: 0.36836910921410465 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:38:27,461] Trial 77 finished with value: 0.37706570856220084 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:39:10,179] Trial 78 finished with value: 0.36818732310370844 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:39:53,770] Trial 79 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:40:38,210] Trial 80 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:41:24,352] Trial 81 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:42:07,946] Trial 82 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:42:51,070] Trial 83 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:43:34,782] Trial 84 finished with value: 0.37408273738375974 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:44:17,866] Trial 85 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:44:25,288] Trial 86 finished with value: 0.36873646806252364 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:45:08,536] Trial 87 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:45:51,511] Trial 88 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:45:56,746] Trial 89 finished with value: 0.37706570856220084 and parameters: {'learning_rate': 0.03, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:46:40,584] Trial 90 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:47:25,676] Trial 91 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:48:08,575] Trial 92 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:48:56,219] Trial 93 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:49:43,597] Trial 94 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:50:31,078] Trial 95 finished with value: 0.36818732310370844 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:51:18,356] Trial 96 finished with value: 0.377219063111122 and parameters: {'learning_rate': 0.1, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:52:05,704] Trial 97 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:52:52,826] Trial 98 finished with value: 0.36836910921410465 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 5}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-02 21:53:00,763] Trial 99 finished with value: 0.36873646806252364 and parameters: {'learning_rate': 0.03, 'depth': 6, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n"
     ]
    }
   ],
   "source": [
    "study_cat = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_cat.optimize(objective_cat, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 모델 학습(데이터 train + val : test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "광주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./gwangju_model/gwangju_rf_opt.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
    "Random Forest - Training Time: 12.96s, RMSE: 0.4227, MSE: 0.1787, MAE: 0.3185, R2: 0.8100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 66.85s, RMSE: 0.4019, MSE: 0.1615, MAE: 0.2958, R2: 0.8282\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./gwangju_model/gwangju_gbm_opt.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.75, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 1000}\n",
      "XGBoost - Training Time: 9.11s, RMSE: 0.4022, MSE: 0.1618, MAE: 0.2982, R2: 0.8280\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./gwangju_model/gwangju_xgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1828\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.267165\n",
      "LightGBM - Training Time: 8.69s, RMSE: 0.4017, MSE: 0.1614, MAE: 0.2970, R2: 0.8284\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./gwangju_model/gwangju_lgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 3}\n",
      "CatBoost - Training Time: 38.93s, RMSE: 0.3983, MSE: 0.1587, MAE: 0.2954, R2: 0.8313\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./gwangju_model/gwangju_cat_opt.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 12.84s, RMSE: 0.4313, MSE: 0.1861, MAE: 0.3223, R2: 0.8177\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./busan_model/busan_rf_opt.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 10, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 46.94s, RMSE: 0.4188, MSE: 0.1754, MAE: 0.3010, R2: 0.8282\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./busan_model/busan_gbm_opt.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 500}\n",
      "XGBoost - Training Time: 9.77s, RMSE: 0.4093, MSE: 0.1676, MAE: 0.2990, R2: 0.8358\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./busan_model/busan_xgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2253\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.253945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 5.86s, RMSE: 0.4072, MSE: 0.1658, MAE: 0.2976, R2: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./busan_model/busan_lgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 5}\n",
      "CatBoost - Training Time: 50.34s, RMSE: 0.4072, MSE: 0.1658, MAE: 0.3008, R2: 0.8376\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./busan_model/busan_cat_opt.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 12.81s, RMSE: 0.3806, MSE: 0.1448, MAE: 0.2757, R2: 0.8341\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./daegu_model/daegu_rf_opt.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 65.31s, RMSE: 0.3753, MSE: 0.1408, MAE: 0.2653, R2: 0.8386\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./daegu_model/daegu_gbm_opt.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.75, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 1000}\n",
      "XGBoost - Training Time: 692.25s, RMSE: 0.3716, MSE: 0.1381, MAE: 0.2672, R2: 0.8418\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./daegu_model/daegu_xgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1890\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.313424\n",
      "LightGBM - Training Time: 39.35s, RMSE: 0.3808, MSE: 0.1450, MAE: 0.2731, R2: 0.8339\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./daegu_model/daegu_lgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}\n",
      "CatBoost - Training Time: 44.24s, RMSE: 0.3695, MSE: 0.1366, MAE: 0.2634, R2: 0.8435\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./daegu_model/daegu_cat_opt.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 12.43s, RMSE: 0.4188, MSE: 0.1754, MAE: 0.3085, R2: 0.8297\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./daejeon_model/daejeon_rf_opt.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 73.20s, RMSE: 0.4045, MSE: 0.1636, MAE: 0.2908, R2: 0.8411\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./daejeon_model/daejeon_gbm_opt.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 10, 'subsample': 0.75, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 1000}\n",
      "XGBoost - Training Time: 20.06s, RMSE: 0.4010, MSE: 0.1608, MAE: 0.2903, R2: 0.8438\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./daejeon_model/daejeon_xgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1677\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.356206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 6.03s, RMSE: 0.4051, MSE: 0.1641, MAE: 0.2927, R2: 0.8406\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./daejeon_model/daejeon_lgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}\n",
      "CatBoost - Training Time: 39.01s, RMSE: 0.4016, MSE: 0.1613, MAE: 0.2904, R2: 0.8434\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./daejeon_model/daejeon_cat_opt.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 12.64s, RMSE: 0.4243, MSE: 0.1801, MAE: 0.3198, R2: 0.7893\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./incheon_model/incheon_rf_opt.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 63.87s, RMSE: 0.4048, MSE: 0.1639, MAE: 0.2957, R2: 0.8082\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./incheon_model/incheon_gbm_opt.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.75, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 1000}\n",
      "XGBoost - Training Time: 9.71s, RMSE: 0.3978, MSE: 0.1583, MAE: 0.2942, R2: 0.8148\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./incheon_model/incheon_xgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1803\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.143254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 38.55s, RMSE: 0.4042, MSE: 0.1633, MAE: 0.3010, R2: 0.8089\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./incheon_model/incheon_lgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}\n",
      "CatBoost - Training Time: 41.99s, RMSE: 0.3953, MSE: 0.1562, MAE: 0.2914, R2: 0.8172\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./incheon_model/incheon_cat_opt.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서울"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 14.46s, RMSE: 0.4369, MSE: 0.1909, MAE: 0.3247, R2: 0.7745\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./seoul_model/seoul_rf_opt.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 73.22s, RMSE: 0.4239, MSE: 0.1797, MAE: 0.3062, R2: 0.7877\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./seoul_model/seoul_gbm_opt.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}\n",
      "XGBoost - Training Time: 47.85s, RMSE: 0.4234, MSE: 0.1793, MAE: 0.3087, R2: 0.7881\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./seoul_model/seoul_xgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Total Bins 1897\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.115901\n",
      "LightGBM - Training Time: 6.20s, RMSE: 0.4218, MSE: 0.1779, MAE: 0.3062, R2: 0.7897\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./seoul_model/seoul_lgb_opt.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}\n",
      "CatBoost - Training Time: 44.73s, RMSE: 0.4175, MSE: 0.1743, MAE: 0.3056, R2: 0.7940\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./seoul_model/seoul_cat_opt.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
