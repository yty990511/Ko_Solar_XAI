{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time\n",
    "import shap\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwangju_preprocessed = pd.read_csv(\"Gwangju_preprocess.csv\")\n",
    "busan_preprocessed = pd.read_csv(\"Busan_preprocess.csv\")\n",
    "daegu_preprocessed = pd.read_csv(\"Daegu_preprocess.csv\")\n",
    "daejeon_preprocessed = pd.read_csv(\"Daejeon_preprocess.csv\")\n",
    "incheon_preprocessed = pd.read_csv(\"Incheon_preprocess.csv\")\n",
    "seoul_preprocessed = pd.read_csv(\"Seoul_preprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp</th>\n",
       "      <th>Humi</th>\n",
       "      <th>WS</th>\n",
       "      <th>WD</th>\n",
       "      <th>Solar</th>\n",
       "      <th>Day_sin</th>\n",
       "      <th>Day_cos</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>...</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "      <th>D1_Temp</th>\n",
       "      <th>D1_Humi</th>\n",
       "      <th>D1_WS</th>\n",
       "      <th>D1_Solar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.8</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.7</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40178</th>\n",
       "      <td>-2.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40179</th>\n",
       "      <td>-1.9</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40180</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.3</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40181</th>\n",
       "      <td>-2.4</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40182</th>\n",
       "      <td>-2.1</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40183 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temp  Humi   WS     WD  Solar   Day_sin   Day_cos  T8  T9  T10  ...  \\\n",
       "0      -7.8  83.0  0.0    0.0   0.00  0.017213  0.999852   1   0    0  ...   \n",
       "1      -5.7  74.0  0.0    0.0   0.22  0.017213  0.999852   0   1    0  ...   \n",
       "2      -3.2  61.0  3.5   50.0   1.13  0.017213  0.999852   0   0    1  ...   \n",
       "3      -2.7  57.0  1.2  360.0   1.70  0.017213  0.999852   0   0    0  ...   \n",
       "4      -1.4  55.0  1.6   50.0   2.00  0.017213  0.999852   0   0    0  ...   \n",
       "...     ...   ...  ...    ...    ...       ...       ...  ..  ..  ...  ...   \n",
       "40178  -2.5  76.0  1.4  230.0   1.36  0.017213  0.999852   0   0    0  ...   \n",
       "40179  -1.9  78.0  1.4  230.0   1.29  0.017213  0.999852   0   0    0  ...   \n",
       "40180  -2.3  86.0  0.5  200.0   0.70  0.017213  0.999852   0   0    0  ...   \n",
       "40181  -2.4  94.0  1.7  230.0   0.26  0.017213  0.999852   0   0    0  ...   \n",
       "40182  -2.1  96.0  1.5  200.0   0.08  0.017213  0.999852   0   0    0  ...   \n",
       "\n",
       "       T13  T14  T15  T16  T17  T18  D1_Temp  D1_Humi  D1_WS  D1_Solar  \n",
       "0        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "1        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "2        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "3        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "4        0    0    0    0    0    0      0.0      0.0    0.0      0.00  \n",
       "...    ...  ...  ...  ...  ...  ...      ...      ...    ...       ...  \n",
       "40178    0    1    0    0    0    0     -5.0     85.0    1.5      0.34  \n",
       "40179    0    0    1    0    0    0     -5.0     86.0    1.4      0.21  \n",
       "40180    0    0    0    1    0    0     -5.3     83.0    2.3      0.18  \n",
       "40181    0    0    0    0    1    0     -5.8     91.0    1.8      0.11  \n",
       "40182    0    0    0    0    0    1     -6.2     93.0    1.0      0.06  \n",
       "\n",
       "[40183 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gwangju_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwangju_train = gwangju_preprocessed.iloc[:24112, :]\n",
    "gwangju_val = gwangju_preprocessed.iloc[24112:32142, :]\n",
    "gwangju_test = gwangju_preprocessed.iloc[32142:, :]\n",
    "gwangju_train_val = gwangju_preprocessed.iloc[:32142, :]\n",
    "\n",
    "busan_train = busan_preprocessed.iloc[:24112, :]\n",
    "busan_val = busan_preprocessed.iloc[24112:32142, :]\n",
    "busan_test = busan_preprocessed.iloc[32142:, :]\n",
    "busan_train_val = busan_preprocessed.iloc[:32142, :]\n",
    "\n",
    "daegu_train = daegu_preprocessed.iloc[:24112, :]\n",
    "daegu_val = daegu_preprocessed.iloc[24112:32142, :]\n",
    "daegu_test = daegu_preprocessed.iloc[32142:, :]\n",
    "daegu_train_val = daegu_preprocessed.iloc[:32142, :]\n",
    "\n",
    "daejeon_train = daejeon_preprocessed.iloc[:24112, :]\n",
    "daejeon_val = daejeon_preprocessed.iloc[24112:32142, :]\n",
    "daejeon_test = daejeon_preprocessed.iloc[32142:, :]\n",
    "daejeon_train_val = daejeon_preprocessed.iloc[:32142, :]\n",
    "\n",
    "incheon_train = incheon_preprocessed.iloc[:24112, :]\n",
    "incheon_val = incheon_preprocessed.iloc[24112:32142, :]\n",
    "incheon_test = incheon_preprocessed.iloc[32142:, :]\n",
    "incheon_train_val = incheon_preprocessed.iloc[:32142, :]\n",
    "\n",
    "seoul_train = seoul_preprocessed.iloc[:24112, :]\n",
    "seoul_val = seoul_preprocessed.iloc[24112:32142, :]\n",
    "seoul_test = seoul_preprocessed.iloc[32142:, :]\n",
    "seoul_train_val = seoul_preprocessed.iloc[:32142, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.모델 입력에 맞게 데이터 변환   (코드 실행시 각 지역을 한작업으로 수행합니다. 총 6작업으로 나누어 진행하였습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지역 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gwangju\n",
    "X_train_rf_gbm = gwangju_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = gwangju_train['Solar']\n",
    "X_val_rf_gbm = gwangju_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = gwangju_val['Solar']\n",
    "X_test_rf_gbm = gwangju_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = gwangju_test['Solar']\n",
    "X_train_val_rf_gbm = gwangju_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = gwangju_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busan\n",
    "X_train_rf_gbm = busan_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = busan_train['Solar']\n",
    "X_val_rf_gbm = busan_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = busan_val['Solar']\n",
    "X_test_rf_gbm = busan_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = busan_test['Solar']\n",
    "X_train_val_rf_gbm = busan_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = busan_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daegu\n",
    "X_train_rf_gbm = daegu_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = daegu_train['Solar']\n",
    "X_val_rf_gbm = daegu_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = daegu_val['Solar']\n",
    "X_test_rf_gbm = daegu_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = daegu_test['Solar']\n",
    "X_train_val_rf_gbm = daegu_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = daegu_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daejeon\n",
    "X_train_rf_gbm = daejeon_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = daejeon_train['Solar']\n",
    "X_val_rf_gbm = daejeon_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = daejeon_val['Solar']\n",
    "X_test_rf_gbm = daejeon_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = daejeon_test['Solar']\n",
    "X_train_val_rf_gbm = daejeon_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = daejeon_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incheon\n",
    "X_train_rf_gbm = incheon_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = incheon_train['Solar']\n",
    "X_val_rf_gbm = incheon_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = incheon_val['Solar']\n",
    "X_test_rf_gbm = incheon_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = incheon_test['Solar']\n",
    "X_train_val_rf_gbm = incheon_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = incheon_train_val['Solar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seoul\n",
    "X_train_rf_gbm = seoul_train.drop(columns=['Solar'])\n",
    "y_train_rf_gbm = seoul_train['Solar']\n",
    "X_val_rf_gbm = seoul_val.drop(columns=['Solar'])\n",
    "y_val_rf_gbm = seoul_val['Solar']\n",
    "X_test_rf_gbm = seoul_test.drop(columns=['Solar'])\n",
    "y_test_rf_gbm = seoul_test['Solar']\n",
    "X_train_val_rf_gbm = seoul_train_val.drop(columns=['Solar'])\n",
    "y_train_val_rf_gbm = seoul_train_val['Solar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 지역 공통"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost를 위한 데이터 준비 공통\n",
    "import xgboost as xgb\n",
    "\n",
    "# 범주형 변수 설정\n",
    "dtrain = xgb.DMatrix(X_train_rf_gbm, label=y_train_rf_gbm)\n",
    "dval = xgb.DMatrix(X_val_rf_gbm, label=y_val_rf_gbm)\n",
    "dtest = xgb.DMatrix(X_test_rf_gbm, label=y_test_rf_gbm)\n",
    "dtrain_val = xgb.DMatrix(X_train_val_rf_gbm, label=y_train_val_rf_gbm)\n",
    "\n",
    "# LightGBM을 위한 데이터 준비 공통\n",
    "import lightgbm as lgb\n",
    "\n",
    "ltrain = lgb.Dataset(X_train_rf_gbm, label=y_train_rf_gbm)\n",
    "lval = lgb.Dataset(X_val_rf_gbm, label=y_val_rf_gbm)\n",
    "ltest = lgb.Dataset(X_test_rf_gbm, label=y_test_rf_gbm)\n",
    "ltrain_val = lgb.Dataset(X_train_val_rf_gbm, label=y_train_val_rf_gbm)\n",
    "\n",
    "# CatBoost를 위한 데이터 준비 공통\n",
    "import catboost as cb\n",
    "\n",
    "ctrain = cb.Pool(data=X_train_rf_gbm, label=y_train_rf_gbm)\n",
    "cval = cb.Pool(data=X_val_rf_gbm, label=y_val_rf_gbm)\n",
    "ctest = cb.Pool(data=X_test_rf_gbm, label=y_test_rf_gbm)\n",
    "ctrain_val = cb.Pool(data=X_train_val_rf_gbm, label=y_train_val_rf_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "     -------------------------------------- 364.4/364.4 KB 2.8 MB/s eta 0:00:00\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\양태영\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Collecting sqlalchemy>=1.4.2\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.5/233.5 KB 2.4 MB/s eta 0:00:00\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     -------------------------------------- 161.8/161.8 KB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\양태영\\appdata\\roaming\\python\\python310\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\양태영\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\양태영\\appdata\\roaming\\python\\python310\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 KB 2.2 MB/s eta 0:00:00\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "     -------------------------------------- 298.4/298.4 KB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\양태영\\appdata\\roaming\\python\\python310\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Installing collected packages: PyYAML, MarkupSafe, greenlet, colorlog, sqlalchemy, Mako, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 MarkupSafe-3.0.2 PyYAML-6.0.2 alembic-1.14.0 colorlog-6.9.0 greenlet-3.1.1 optuna-4.1.0 sqlalchemy-2.0.36\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script mako-render.exe is installed in 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script optuna.exe is installed in 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\양태영\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna + seed 고정 / train으로 학습 val을 통해 평가\n",
    "\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42  # 랜덤 시드 설정 추가\n",
    "\n",
    "# Random Forest Objective\n",
    "def objective_rf(trial):\n",
    "    \n",
    "    np.random.seed(SEED)  # 랜덤 시드 설정 추가\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [128]),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, random_state=SEED)\n",
    "    model.fit(X_train_rf_gbm, y_train_rf_gbm)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# Gradient Boosting Objective\n",
    "def objective_gbm(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 500]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, step=0.01),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'loss': trial.suggest_categorical('loss', ['quantile', 'huber'])\n",
    "    }\n",
    "    model = GradientBoostingRegressor(**params, random_state=SEED)\n",
    "    model.fit(X_train_rf_gbm, y_train_rf_gbm)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# XGBoost Objective for Optuna\n",
    "def objective_xgb(trial):\n",
    "    # Hyperparameter suggestion\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, step=0.01),\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0, step = 0.05),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0, step = 0.05),\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
    "        'random_state': SEED  # XGBoost의 랜덤 시드 설정 추가\n",
    "    }\n",
    "    # Separate num_boost_round from params\n",
    "    num_boost_round = trial.suggest_categorical('n_estimators', [250, 500, 1000])\n",
    "\n",
    "    # XGBoost Training with DMatrix\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = model.predict(dval)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(dval.get_label(), predictions))\n",
    "    return rmse\n",
    "\n",
    "# LightGBM Objective\n",
    "def objective_lgb(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, step=0.01),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 64, 64),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.5]),\n",
    "        'feature_fraction': trial.suggest_categorical('feature_fraction', [1.0]),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "        'seed': SEED  # LightGBM의 랜덤 시드 설정 추가\n",
    "    }\n",
    "    num_boost_round = trial.suggest_categorical('n_estimators', [1000, 1500])\n",
    "    model = lgb.train(params, ltrain, num_boost_round=num_boost_round)\n",
    "    predictions = model.predict(X_val_rf_gbm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse\n",
    "\n",
    "# CatBoost Objective\n",
    "def objective_cat(trial):\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1, step = 0.01),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 9),\n",
    "        'random_state': SEED\n",
    "    }\n",
    "    model = cb.CatBoostRegressor(**params,verbose=0)\n",
    "    model.fit(ctrain)\n",
    "    predictions = model.predict(cval)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_rf_gbm, predictions))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Optuna 하이퍼파라미터 최적화(train : val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 19:14:52,178] A new study created in memory with name: no-name-15d08627-4007-4466-9b49-dbd767bd0674\n",
      "[I 2024-12-05 19:14:58,038] Trial 0 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:04,020] Trial 1 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:09,843] Trial 2 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:15,830] Trial 3 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:22,313] Trial 4 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:28,207] Trial 5 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:34,196] Trial 6 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:40,158] Trial 7 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:46,127] Trial 8 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:52,058] Trial 9 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:15:58,436] Trial 10 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:04,285] Trial 11 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:10,130] Trial 12 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:15,937] Trial 13 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:21,676] Trial 14 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:27,475] Trial 15 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:33,225] Trial 16 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:38,998] Trial 17 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:44,760] Trial 18 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:50,527] Trial 19 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:16:56,259] Trial 20 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:02,076] Trial 21 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:07,861] Trial 22 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:13,625] Trial 23 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:19,359] Trial 24 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:25,126] Trial 25 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:30,910] Trial 26 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:36,725] Trial 27 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:42,476] Trial 28 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:48,242] Trial 29 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:54,029] Trial 30 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:17:59,776] Trial 31 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:05,542] Trial 32 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:11,308] Trial 33 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:17,042] Trial 34 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:22,794] Trial 35 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:28,560] Trial 36 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:34,376] Trial 37 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:40,159] Trial 38 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:45,910] Trial 39 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:51,659] Trial 40 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:18:57,426] Trial 41 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:03,226] Trial 42 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:09,000] Trial 43 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:14,727] Trial 44 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:20,475] Trial 45 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:26,703] Trial 46 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:32,459] Trial 47 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:38,210] Trial 48 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:43,959] Trial 49 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:49,710] Trial 50 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:19:55,543] Trial 51 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:01,309] Trial 52 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:07,043] Trial 53 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:12,808] Trial 54 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:18,610] Trial 55 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:24,342] Trial 56 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:30,143] Trial 57 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:35,907] Trial 58 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:41,643] Trial 59 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:47,408] Trial 60 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:53,125] Trial 61 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:20:58,897] Trial 62 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:04,676] Trial 63 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:10,427] Trial 64 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:16,209] Trial 65 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:21,976] Trial 66 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:27,724] Trial 67 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:33,504] Trial 68 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:39,258] Trial 69 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:45,007] Trial 70 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:50,709] Trial 71 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:21:56,409] Trial 72 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:02,127] Trial 73 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:07,843] Trial 74 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:13,532] Trial 75 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:19,226] Trial 76 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:24,926] Trial 77 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:30,609] Trial 78 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:36,326] Trial 79 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:42,008] Trial 80 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:47,691] Trial 81 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:53,376] Trial 82 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:22:59,074] Trial 83 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:04,742] Trial 84 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:10,409] Trial 85 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:16,610] Trial 86 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:22,301] Trial 87 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:28,003] Trial 88 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:33,693] Trial 89 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:39,359] Trial 90 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:45,106] Trial 91 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:50,799] Trial 92 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:23:56,495] Trial 93 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:24:02,193] Trial 94 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:24:07,860] Trial 95 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:24:13,559] Trial 96 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:24:19,242] Trial 97 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:24:25,010] Trial 98 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.3892836685749838.\n",
      "[I 2024-12-05 19:24:30,742] Trial 99 finished with value: 0.3892836685749838 and parameters: {'n_estimators': 128, 'max_features': 'log2'}. Best is trial 0 with value: 0.3892836685749838.\n"
     ]
    }
   ],
   "source": [
    "study_rf = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_rf.optimize(objective_rf, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 19:24:30,759] A new study created in memory with name: no-name-762ed100-5321-4147-aa9d-fad436e1fad3\n",
      "[I 2024-12-05 19:25:16,296] Trial 0 finished with value: 0.46962303995892607 and parameters: {'n_estimators': 500, 'learning_rate': 0.08, 'max_depth': 8, 'loss': 'quantile'}. Best is trial 0 with value: 0.46962303995892607.\n",
      "[I 2024-12-05 19:26:34,176] Trial 1 finished with value: 0.3795400549909196 and parameters: {'n_estimators': 500, 'learning_rate': 0.06999999999999999, 'max_depth': 9, 'loss': 'huber'}. Best is trial 1 with value: 0.3795400549909196.\n",
      "[I 2024-12-05 19:26:42,301] Trial 2 finished with value: 0.4402242209687781 and parameters: {'n_estimators': 100, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 1 with value: 0.3795400549909196.\n",
      "[I 2024-12-05 19:26:48,559] Trial 3 finished with value: 0.3778112418110285 and parameters: {'n_estimators': 100, 'learning_rate': 0.06999999999999999, 'max_depth': 5, 'loss': 'huber'}. Best is trial 3 with value: 0.3778112418110285.\n",
      "[I 2024-12-05 19:27:39,976] Trial 4 finished with value: 0.4818337346766053 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 8, 'loss': 'quantile'}. Best is trial 3 with value: 0.3778112418110285.\n",
      "[I 2024-12-05 19:27:57,141] Trial 5 finished with value: 0.9721359857203314 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 10, 'loss': 'quantile'}. Best is trial 3 with value: 0.3778112418110285.\n",
      "[I 2024-12-05 19:28:07,209] Trial 6 finished with value: 0.3692922280761091 and parameters: {'n_estimators': 100, 'learning_rate': 0.06999999999999999, 'max_depth': 7, 'loss': 'huber'}. Best is trial 6 with value: 0.3692922280761091.\n",
      "[I 2024-12-05 19:29:08,906] Trial 7 finished with value: 0.3701857213967711 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 8, 'loss': 'huber'}. Best is trial 6 with value: 0.3692922280761091.\n",
      "[I 2024-12-05 19:29:21,195] Trial 8 finished with value: 0.4756574385419639 and parameters: {'n_estimators': 100, 'learning_rate': 0.09999999999999999, 'max_depth': 9, 'loss': 'quantile'}. Best is trial 6 with value: 0.3692922280761091.\n",
      "[I 2024-12-05 19:30:01,405] Trial 9 finished with value: 0.3785752461737557 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 6, 'loss': 'huber'}. Best is trial 6 with value: 0.3692922280761091.\n",
      "[I 2024-12-05 19:30:09,409] Trial 10 finished with value: 0.3779715991056674 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 6, 'loss': 'huber'}. Best is trial 6 with value: 0.3692922280761091.\n",
      "[I 2024-12-05 19:30:57,826] Trial 11 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 11 with value: 0.36818169496954045.\n",
      "[I 2024-12-05 19:31:46,209] Trial 12 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 11 with value: 0.36818169496954045.\n",
      "[I 2024-12-05 19:32:34,542] Trial 13 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 11 with value: 0.36818169496954045.\n",
      "[I 2024-12-05 19:33:05,327] Trial 14 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 14 with value: 0.36626076814933495.\n",
      "[I 2024-12-05 19:33:36,107] Trial 15 finished with value: 0.36626076814933495 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'huber'}. Best is trial 14 with value: 0.36626076814933495.\n",
      "[I 2024-12-05 19:34:06,860] Trial 16 finished with value: 0.3670092453582597 and parameters: {'n_estimators': 500, 'learning_rate': 0.060000000000000005, 'max_depth': 5, 'loss': 'huber'}. Best is trial 14 with value: 0.36626076814933495.\n",
      "[I 2024-12-05 19:34:37,596] Trial 17 finished with value: 0.37021511017697606 and parameters: {'n_estimators': 500, 'learning_rate': 0.09, 'max_depth': 5, 'loss': 'huber'}. Best is trial 14 with value: 0.36626076814933495.\n",
      "[I 2024-12-05 19:35:05,934] Trial 18 finished with value: 0.4868341985984668 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 14 with value: 0.36626076814933495.\n",
      "[I 2024-12-05 19:35:45,043] Trial 19 finished with value: 0.36883695462204863 and parameters: {'n_estimators': 500, 'learning_rate': 0.060000000000000005, 'max_depth': 6, 'loss': 'huber'}. Best is trial 14 with value: 0.36626076814933495.\n",
      "[I 2024-12-05 19:36:16,317] Trial 20 finished with value: 0.3657129161635212 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 5, 'loss': 'huber'}. Best is trial 20 with value: 0.3657129161635212.\n",
      "[I 2024-12-05 19:36:47,534] Trial 21 finished with value: 0.3657129161635212 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 5, 'loss': 'huber'}. Best is trial 20 with value: 0.3657129161635212.\n",
      "[I 2024-12-05 19:37:27,735] Trial 22 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:38:07,374] Trial 23 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:38:46,713] Trial 24 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:39:28,149] Trial 25 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:40:03,961] Trial 26 finished with value: 0.4820016611305101 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:40:54,659] Trial 27 finished with value: 0.36623140451108066 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:41:33,656] Trial 28 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:42:18,187] Trial 29 finished with value: 0.5225299386558436 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 7, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:42:57,235] Trial 30 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:43:36,333] Trial 31 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:44:15,827] Trial 32 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:44:54,837] Trial 33 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:45:43,888] Trial 34 finished with value: 0.3666688047630493 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:45:58,144] Trial 35 finished with value: 0.4202794356762475 and parameters: {'n_estimators': 100, 'learning_rate': 0.02, 'max_depth': 8, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:46:38,253] Trial 36 finished with value: 0.3785752461737557 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:46:52,307] Trial 37 finished with value: 0.7478953402178597 and parameters: {'n_estimators': 100, 'learning_rate': 0.02, 'max_depth': 9, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:47:41,545] Trial 38 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:49:27,416] Trial 39 finished with value: 0.3777638649299255 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 10, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:49:34,635] Trial 40 finished with value: 0.5255281120886134 and parameters: {'n_estimators': 100, 'learning_rate': 0.060000000000000005, 'max_depth': 6, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:50:13,779] Trial 41 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:50:52,804] Trial 42 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:51:32,282] Trial 43 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:52:20,984] Trial 44 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:52:59,728] Trial 45 finished with value: 0.3717383537816045 and parameters: {'n_estimators': 500, 'learning_rate': 0.08, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:53:06,183] Trial 46 finished with value: 0.5731386156067861 and parameters: {'n_estimators': 100, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:54:08,327] Trial 47 finished with value: 0.3701857213967711 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 8, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:54:47,243] Trial 48 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:55:35,770] Trial 49 finished with value: 0.36917647417385374 and parameters: {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:56:07,635] Trial 50 finished with value: 0.3705581090280771 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:56:46,729] Trial 51 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:57:25,920] Trial 52 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:58:04,877] Trial 53 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:58:54,605] Trial 54 finished with value: 0.36623140451108066 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 19:59:25,863] Trial 55 finished with value: 0.3657129161635212 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:00:04,885] Trial 56 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:00:13,852] Trial 57 finished with value: 0.5234211957710873 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 7, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:00:45,116] Trial 58 finished with value: 0.3657129161635212 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:01:23,778] Trial 59 finished with value: 0.3733268555030784 and parameters: {'n_estimators': 500, 'learning_rate': 0.09999999999999999, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:01:55,326] Trial 60 finished with value: 0.38994006105056395 and parameters: {'n_estimators': 500, 'learning_rate': 0.01, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:02:34,502] Trial 61 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:03:14,245] Trial 62 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:03:53,746] Trial 63 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:04:32,668] Trial 64 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:05:11,761] Trial 65 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:06:01,479] Trial 66 finished with value: 0.36623140451108066 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:06:36,835] Trial 67 finished with value: 0.4820016611305101 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:07:15,859] Trial 68 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:07:47,112] Trial 69 finished with value: 0.3705581090280771 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:07:57,454] Trial 70 finished with value: 0.3728657423281418 and parameters: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:08:36,546] Trial 71 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:09:15,587] Trial 72 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:09:54,587] Trial 73 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:10:33,744] Trial 74 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:11:13,387] Trial 75 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:12:37,283] Trial 76 finished with value: 0.3710937664454051 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 9, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:13:08,602] Trial 77 finished with value: 0.3705581090280771 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:13:42,750] Trial 78 finished with value: 0.4768091910990365 and parameters: {'n_estimators': 500, 'learning_rate': 0.06999999999999999, 'max_depth': 6, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:14:31,350] Trial 79 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:14:39,470] Trial 80 finished with value: 0.4031245878563227 and parameters: {'n_estimators': 100, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:15:18,736] Trial 81 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:15:57,849] Trial 82 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:16:37,505] Trial 83 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:17:16,486] Trial 84 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:17:55,880] Trial 85 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:18:34,862] Trial 86 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:19:05,995] Trial 87 finished with value: 0.3657129161635212 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 5, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:19:54,978] Trial 88 finished with value: 0.3666688047630493 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:20:34,476] Trial 89 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:21:03,004] Trial 90 finished with value: 0.4886900117135741 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 5, 'loss': 'quantile'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:21:42,055] Trial 91 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:22:21,145] Trial 92 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:23:00,279] Trial 93 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:23:39,748] Trial 94 finished with value: 0.3664340667511423 and parameters: {'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:24:19,446] Trial 95 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:24:58,469] Trial 96 finished with value: 0.3670556020069749 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:25:37,586] Trial 97 finished with value: 0.36539818686434716 and parameters: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:25:45,684] Trial 98 finished with value: 0.4402242209687781 and parameters: {'n_estimators': 100, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n",
      "[I 2024-12-05 20:26:34,404] Trial 99 finished with value: 0.36818169496954045 and parameters: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 7, 'loss': 'huber'}. Best is trial 22 with value: 0.36539818686434716.\n"
     ]
    }
   ],
   "source": [
    "study_gbm = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_gbm.optimize(objective_gbm, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 20:26:34,432] A new study created in memory with name: no-name-24b7fd49-557a-4064-aae5-3adac452aee1\n",
      "[I 2024-12-05 20:26:38,445] Trial 0 finished with value: 0.37635311484336853 and parameters: {'learning_rate': 0.04, 'max_depth': 10, 'subsample': 0.9, 'colsample_bytree': 0.8, 'booster': 'gbtree', 'n_estimators': 500}. Best is trial 0 with value: 0.37635311484336853.\n",
      "[I 2024-12-05 20:26:40,546] Trial 1 finished with value: 0.3808160126209259 and parameters: {'learning_rate': 0.08, 'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 0 with value: 0.37635311484336853.\n",
      "[I 2024-12-05 20:27:39,455] Trial 2 finished with value: 0.3749759793281555 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.55, 'booster': 'dart', 'n_estimators': 500}. Best is trial 2 with value: 0.3749759793281555.\n",
      "[I 2024-12-05 20:27:41,427] Trial 3 finished with value: 0.3812463581562042 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 8, 'subsample': 0.5, 'colsample_bytree': 0.8, 'booster': 'gbtree', 'n_estimators': 500}. Best is trial 2 with value: 0.3749759793281555.\n",
      "[I 2024-12-05 20:28:40,562] Trial 4 finished with value: 0.3699679672718048 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.8500000000000001, 'colsample_bytree': 0.7, 'booster': 'dart', 'n_estimators': 500}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:29:39,489] Trial 5 finished with value: 0.37691280245780945 and parameters: {'learning_rate': 0.06999999999999999, 'max_depth': 7, 'subsample': 0.75, 'colsample_bytree': 0.8, 'booster': 'dart', 'n_estimators': 500}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:33:59,946] Trial 6 finished with value: 0.3888219892978668 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 10, 'subsample': 0.5, 'colsample_bytree': 0.6, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:34:00,734] Trial 7 finished with value: 0.3747895061969757 and parameters: {'learning_rate': 0.04, 'max_depth': 7, 'subsample': 0.75, 'colsample_bytree': 0.55, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:34:10,128] Trial 8 finished with value: 0.3711366057395935 and parameters: {'learning_rate': 0.01, 'max_depth': 10, 'subsample': 0.8500000000000001, 'colsample_bytree': 0.9, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:35:09,186] Trial 9 finished with value: 0.3815062344074249 and parameters: {'learning_rate': 0.06999999999999999, 'max_depth': 7, 'subsample': 0.5, 'colsample_bytree': 0.65, 'booster': 'dart', 'n_estimators': 500}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:35:23,833] Trial 10 finished with value: 0.375266432762146 and parameters: {'learning_rate': 0.09999999999999999, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 0.7, 'booster': 'dart', 'n_estimators': 250}. Best is trial 4 with value: 0.3699679672718048.\n",
      "[I 2024-12-05 20:35:30,358] Trial 11 finished with value: 0.3695451319217682 and parameters: {'learning_rate': 0.01, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 11 with value: 0.3695451319217682.\n",
      "[I 2024-12-05 20:35:37,256] Trial 12 finished with value: 0.37117889523506165 and parameters: {'learning_rate': 0.01, 'max_depth': 9, 'subsample': 0.95, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 11 with value: 0.3695451319217682.\n",
      "[I 2024-12-05 20:39:46,413] Trial 13 finished with value: 0.3754614591598511 and parameters: {'learning_rate': 0.03, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 11 with value: 0.3695451319217682.\n",
      "[I 2024-12-05 20:39:48,236] Trial 14 finished with value: 0.37516650557518005 and parameters: {'learning_rate': 0.02, 'max_depth': 9, 'subsample': 0.65, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 11 with value: 0.3695451319217682.\n",
      "[I 2024-12-05 20:40:52,442] Trial 15 finished with value: 0.3695048689842224 and parameters: {'learning_rate': 0.02, 'max_depth': 8, 'subsample': 0.8500000000000001, 'colsample_bytree': 0.7, 'booster': 'dart', 'n_estimators': 500}. Best is trial 15 with value: 0.3695048689842224.\n",
      "[I 2024-12-05 20:44:51,932] Trial 16 finished with value: 0.3724522590637207 and parameters: {'learning_rate': 0.02, 'max_depth': 8, 'subsample': 1.0, 'colsample_bytree': 0.8500000000000001, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 15 with value: 0.3695048689842224.\n",
      "[I 2024-12-05 20:44:55,515] Trial 17 finished with value: 0.3689860999584198 and parameters: {'learning_rate': 0.02, 'max_depth': 9, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 500}. Best is trial 17 with value: 0.3689860999584198.\n",
      "[I 2024-12-05 20:44:57,812] Trial 18 finished with value: 0.37243351340293884 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.65, 'colsample_bytree': 0.65, 'booster': 'gbtree', 'n_estimators': 500}. Best is trial 17 with value: 0.3689860999584198.\n",
      "[I 2024-12-05 20:46:02,663] Trial 19 finished with value: 0.3760524094104767 and parameters: {'learning_rate': 0.02, 'max_depth': 9, 'subsample': 0.6, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 500}. Best is trial 17 with value: 0.3689860999584198.\n",
      "[I 2024-12-05 20:47:05,779] Trial 20 finished with value: 0.36750221252441406 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:48:08,144] Trial 21 finished with value: 0.36750221252441406 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:49:14,031] Trial 22 finished with value: 0.36750221252441406 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:50:17,386] Trial 23 finished with value: 0.37021681666374207 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:51:20,013] Trial 24 finished with value: 0.37430548667907715 and parameters: {'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:52:19,221] Trial 25 finished with value: 0.3692077398300171 and parameters: {'learning_rate': 0.04, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:52:38,530] Trial 26 finished with value: 0.368607759475708 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:53:44,756] Trial 27 finished with value: 0.371315598487854 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.75, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:54:52,756] Trial 28 finished with value: 0.3693200945854187 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.55, 'colsample_bytree': 0.8500000000000001, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:55:55,115] Trial 29 finished with value: 0.3718956410884857 and parameters: {'learning_rate': 0.04, 'max_depth': 8, 'subsample': 0.8, 'colsample_bytree': 0.8500000000000001, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:57:00,280] Trial 30 finished with value: 0.3827105462551117 and parameters: {'learning_rate': 0.09999999999999999, 'max_depth': 9, 'subsample': 0.8, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 500}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:57:18,776] Trial 31 finished with value: 0.368607759475708 and parameters: {'learning_rate': 0.03, 'max_depth': 8, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:57:34,937] Trial 32 finished with value: 0.3684828281402588 and parameters: {'learning_rate': 0.04, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:57:51,466] Trial 33 finished with value: 0.36813464760780334 and parameters: {'learning_rate': 0.04, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 20 with value: 0.36750221252441406.\n",
      "[I 2024-12-05 20:58:07,095] Trial 34 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 34 with value: 0.36742323637008667.\n",
      "[I 2024-12-05 20:58:21,878] Trial 35 finished with value: 0.3672652244567871 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 20:58:36,471] Trial 36 finished with value: 0.3674309253692627 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 20:58:51,355] Trial 37 finished with value: 0.3693073093891144 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 6, 'subsample': 0.55, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 20:59:06,724] Trial 38 finished with value: 0.37001749873161316 and parameters: {'learning_rate': 0.06999999999999999, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 0.8500000000000001, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 20:59:21,347] Trial 39 finished with value: 0.370877206325531 and parameters: {'learning_rate': 0.08, 'max_depth': 6, 'subsample': 0.55, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 20:59:35,972] Trial 40 finished with value: 0.36831992864608765 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 0.8, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 20:59:51,621] Trial 41 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:00:06,938] Trial 42 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:00:22,452] Trial 43 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:00:37,606] Trial 44 finished with value: 0.3679335415363312 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:00:52,855] Trial 45 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:01:08,229] Trial 46 finished with value: 0.36904650926589966 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.55, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:01:23,554] Trial 47 finished with value: 0.37207868695259094 and parameters: {'learning_rate': 0.06999999999999999, 'max_depth': 7, 'subsample': 0.75, 'colsample_bytree': 0.8, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:01:24,487] Trial 48 finished with value: 0.36994317173957825 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:01:39,854] Trial 49 finished with value: 0.3679896891117096 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.8500000000000001, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:01:54,356] Trial 50 finished with value: 0.3695477247238159 and parameters: {'learning_rate': 0.06999999999999999, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:02:10,076] Trial 51 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:02:25,689] Trial 52 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:02:41,080] Trial 53 finished with value: 0.369527131319046 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 35 with value: 0.3672652244567871.\n",
      "[I 2024-12-05 21:02:41,753] Trial 54 finished with value: 0.3670087158679962 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:42,359] Trial 55 finished with value: 0.37175992131233215 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.75, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:44,744] Trial 56 finished with value: 0.37253886461257935 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.55, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:45,496] Trial 57 finished with value: 0.369623601436615 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.9, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:46,212] Trial 58 finished with value: 0.36718958616256714 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:46,963] Trial 59 finished with value: 0.3691359758377075 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:47,663] Trial 60 finished with value: 0.367567241191864 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.75, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:48,643] Trial 61 finished with value: 0.36889296770095825 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:49,383] Trial 62 finished with value: 0.3679361045360565 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:50,360] Trial 63 finished with value: 0.36994317173957825 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:54,604] Trial 64 finished with value: 0.3782186806201935 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.6, 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:55,517] Trial 65 finished with value: 0.3704322278499603 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:02:56,207] Trial 66 finished with value: 0.36959338188171387 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 0.9, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:03:12,449] Trial 67 finished with value: 0.3690817654132843 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.55, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:03:13,176] Trial 68 finished with value: 0.36903253197669983 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.7, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:07:13,647] Trial 69 finished with value: 0.37859347462654114 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:07:29,455] Trial 70 finished with value: 0.36806827783584595 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 0.8500000000000001, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:07:45,285] Trial 71 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:08:01,517] Trial 72 finished with value: 0.3692857027053833 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:08:17,455] Trial 73 finished with value: 0.3700098395347595 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:08:33,366] Trial 74 finished with value: 0.3679335415363312 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:08:52,405] Trial 75 finished with value: 0.3779543340206146 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 10, 'subsample': 0.65, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:09:08,613] Trial 76 finished with value: 0.3732593059539795 and parameters: {'learning_rate': 0.08, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:09:23,783] Trial 77 finished with value: 0.36802369356155396 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:09:39,640] Trial 78 finished with value: 0.37570667266845703 and parameters: {'learning_rate': 0.04, 'max_depth': 7, 'subsample': 0.7, 'colsample_bytree': 0.5, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:09:54,667] Trial 79 finished with value: 0.3680310547351837 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.55, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:09:55,563] Trial 80 finished with value: 0.37301281094551086 and parameters: {'learning_rate': 0.09, 'max_depth': 7, 'subsample': 0.95, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:10:11,688] Trial 81 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:10:27,198] Trial 82 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:10:42,875] Trial 83 finished with value: 0.3677753806114197 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:11:00,359] Trial 84 finished with value: 0.37003186345100403 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:11:16,041] Trial 85 finished with value: 0.36898669600486755 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:11:30,950] Trial 86 finished with value: 0.37012702226638794 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.7, 'colsample_bytree': 0.7, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:15:20,897] Trial 87 finished with value: 0.3724505305290222 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.6, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 1000}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:15:36,873] Trial 88 finished with value: 0.369527131319046 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:15:52,088] Trial 89 finished with value: 0.36780598759651184 and parameters: {'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.65, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:15:53,371] Trial 90 finished with value: 0.3696238696575165 and parameters: {'learning_rate': 0.04, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.95, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:16:09,274] Trial 91 finished with value: 0.36742323637008667 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.65, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:16:25,016] Trial 92 finished with value: 0.3671668767929077 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:16:40,728] Trial 93 finished with value: 0.36795321106910706 and parameters: {'learning_rate': 0.060000000000000005, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 0.9, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:16:56,399] Trial 94 finished with value: 0.3680016100406647 and parameters: {'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.55, 'colsample_bytree': 0.95, 'booster': 'dart', 'n_estimators': 250}. Best is trial 54 with value: 0.3670087158679962.\n",
      "[I 2024-12-05 21:17:12,247] Trial 95 finished with value: 0.3661382496356964 and parameters: {'learning_rate': 0.04, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 95 with value: 0.3661382496356964.\n",
      "[I 2024-12-05 21:17:28,737] Trial 96 finished with value: 0.36899295449256897 and parameters: {'learning_rate': 0.04, 'max_depth': 8, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 95 with value: 0.3661382496356964.\n",
      "[I 2024-12-05 21:17:44,492] Trial 97 finished with value: 0.36806055903434753 and parameters: {'learning_rate': 0.03, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}. Best is trial 95 with value: 0.3661382496356964.\n",
      "[I 2024-12-05 21:17:45,172] Trial 98 finished with value: 0.36711812019348145 and parameters: {'learning_rate': 0.04, 'max_depth': 6, 'subsample': 0.55, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 95 with value: 0.3661382496356964.\n",
      "[I 2024-12-05 21:17:47,552] Trial 99 finished with value: 0.368867427110672 and parameters: {'learning_rate': 0.03, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 1.0, 'booster': 'gbtree', 'n_estimators': 250}. Best is trial 95 with value: 0.3661382496356964.\n"
     ]
    }
   ],
   "source": [
    "study_xgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_xgb.optimize(objective_xgb, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:17:47,573] A new study created in memory with name: no-name-1138157f-a12b-47ab-90ff-c1ab44e8328f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:17:50,634] Trial 0 finished with value: 0.37408119323378836 and parameters: {'learning_rate': 0.04, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 0 with value: 0.37408119323378836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:18:15,446] Trial 1 finished with value: 0.39065045840043805 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 0 with value: 0.37408119323378836.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:18:18,047] Trial 2 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:18:32,302] Trial 3 finished with value: 0.40515538126750045 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:18:55,438] Trial 4 finished with value: 0.3740864906239625 and parameters: {'learning_rate': 0.06999999999999999, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:09,498] Trial 5 finished with value: 0.3749341948320394 and parameters: {'learning_rate': 0.08, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:12,863] Trial 6 finished with value: 0.3806883440804288 and parameters: {'learning_rate': 0.06999999999999999, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:15,142] Trial 7 finished with value: 0.38126544500201737 and parameters: {'learning_rate': 0.09, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:17,783] Trial 8 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:31,804] Trial 9 finished with value: 0.37497572295852494 and parameters: {'learning_rate': 0.06999999999999999, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:35,318] Trial 10 finished with value: 0.376667565220932 and parameters: {'learning_rate': 0.04, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:38,017] Trial 11 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:40,927] Trial 12 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:43,480] Trial 13 finished with value: 0.37408119323378836 and parameters: {'learning_rate': 0.04, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:46,305] Trial 14 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:48,862] Trial 15 finished with value: 0.3718602617762223 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:51,130] Trial 16 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:54,231] Trial 17 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:19:57,894] Trial 18 finished with value: 0.37441159216125375 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:00,095] Trial 19 finished with value: 0.3826400719301149 and parameters: {'learning_rate': 0.09999999999999999, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:02,843] Trial 20 finished with value: 0.376072387725667 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:05,580] Trial 21 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:08,691] Trial 22 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:11,251] Trial 23 finished with value: 0.3718602617762223 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:13,718] Trial 24 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:16,938] Trial 25 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:40,406] Trial 26 finished with value: 0.3808346361019479 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000803 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:43,070] Trial 27 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:45,399] Trial 28 finished with value: 0.3772509817950232 and parameters: {'learning_rate': 0.060000000000000005, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:48,281] Trial 29 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:50,731] Trial 30 finished with value: 0.37408119323378836 and parameters: {'learning_rate': 0.04, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:53,510] Trial 31 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:56,189] Trial 32 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:20:59,141] Trial 33 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:21:24,626] Trial 34 finished with value: 0.39065045840043805 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:21:38,849] Trial 35 finished with value: 0.40515538126750045 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:21:41,383] Trial 36 finished with value: 0.3718602617762223 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:21:45,376] Trial 37 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:00,023] Trial 38 finished with value: 0.40515538126750045 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:02,950] Trial 39 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:05,381] Trial 40 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:08,266] Trial 41 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000962 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:11,397] Trial 42 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:13,999] Trial 43 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:16,485] Trial 44 finished with value: 0.3795467669167207 and parameters: {'learning_rate': 0.08, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:20,183] Trial 45 finished with value: 0.37441159216125375 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:34,716] Trial 46 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:37,277] Trial 47 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:39,990] Trial 48 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:42,716] Trial 49 finished with value: 0.37408119323378836 and parameters: {'learning_rate': 0.04, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:46,148] Trial 50 finished with value: 0.3800558754952854 and parameters: {'learning_rate': 0.060000000000000005, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:48,877] Trial 51 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:52,331] Trial 52 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:54,792] Trial 53 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:22:57,514] Trial 54 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:00,122] Trial 55 finished with value: 0.3718602617762223 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:02,702] Trial 56 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:05,700] Trial 57 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:19,535] Trial 58 finished with value: 0.3740372775795542 and parameters: {'learning_rate': 0.09, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:22,190] Trial 59 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:24,877] Trial 60 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:28,308] Trial 61 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:31,173] Trial 62 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:33,855] Trial 63 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:36,525] Trial 64 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:39,149] Trial 65 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:42,617] Trial 66 finished with value: 0.37879070648719537 and parameters: {'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:45,510] Trial 67 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:23:59,713] Trial 68 finished with value: 0.38793840843473987 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:04,598] Trial 69 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:07,462] Trial 70 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:10,277] Trial 71 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:13,174] Trial 72 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:15,668] Trial 73 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:18,464] Trial 74 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:21,142] Trial 75 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:25,862] Trial 76 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:28,483] Trial 77 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:31,127] Trial 78 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:45,553] Trial 79 finished with value: 0.38793840843473987 and parameters: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:48,381] Trial 80 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:51,099] Trial 81 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:54,233] Trial 82 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:57,389] Trial 83 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:24:59,887] Trial 84 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:02,683] Trial 85 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:05,090] Trial 86 finished with value: 0.3784890343348734 and parameters: {'learning_rate': 0.06999999999999999, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:08,900] Trial 87 finished with value: 0.3672010780843957 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1500}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:11,485] Trial 88 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:14,134] Trial 89 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:28,883] Trial 90 finished with value: 0.46839311734161215 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:31,943] Trial 91 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:34,855] Trial 92 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:37,751] Trial 93 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:40,527] Trial 94 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:42,826] Trial 95 finished with value: 0.3795467669167207 and parameters: {'learning_rate': 0.08, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:45,483] Trial 96 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:48,185] Trial 97 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:51,044] Trial 98 finished with value: 0.366419897466168 and parameters: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1840\n",
      "[LightGBM] [Info] Number of data points in the train set: 24112, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.093214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:53,557] Trial 99 finished with value: 0.36796978697235616 and parameters: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}. Best is trial 2 with value: 0.366419897466168.\n"
     ]
    }
   ],
   "source": [
    "study_lgb = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_lgb.optimize(objective_lgb, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-05 21:25:53,565] A new study created in memory with name: no-name-d1a79d8d-054f-4277-99cc-927eeefa7411\n",
      "[I 2024-12-05 21:26:13,271] Trial 0 finished with value: 0.3700971815157761 and parameters: {'learning_rate': 0.05, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 0 with value: 0.3700971815157761.\n",
      "[I 2024-12-05 21:26:17,921] Trial 1 finished with value: 0.3704788625340517 and parameters: {'learning_rate': 0.07, 'depth': 5, 'l2_leaf_reg': 2}. Best is trial 0 with value: 0.3700971815157761.\n",
      "[I 2024-12-05 21:26:37,197] Trial 2 finished with value: 0.36798241824514444 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 6}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:26:40,305] Trial 3 finished with value: 0.372522275676749 and parameters: {'learning_rate': 0.08, 'depth': 4, 'l2_leaf_reg': 9}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:26:43,730] Trial 4 finished with value: 0.37202741693052227 and parameters: {'learning_rate': 0.09, 'depth': 5, 'l2_leaf_reg': 2}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:26:47,723] Trial 5 finished with value: 0.3691536288621523 and parameters: {'learning_rate': 0.04, 'depth': 6, 'l2_leaf_reg': 5}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:26:51,580] Trial 6 finished with value: 0.368461890756453 and parameters: {'learning_rate': 0.06, 'depth': 6, 'l2_leaf_reg': 6}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:26:55,363] Trial 7 finished with value: 0.3680422442532819 and parameters: {'learning_rate': 0.04, 'depth': 6, 'l2_leaf_reg': 4}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:27:04,864] Trial 8 finished with value: 0.37224907964325515 and parameters: {'learning_rate': 0.06, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:27:11,196] Trial 9 finished with value: 0.372612967367242 and parameters: {'learning_rate': 0.07, 'depth': 8, 'l2_leaf_reg': 1}. Best is trial 2 with value: 0.36798241824514444.\n",
      "[I 2024-12-05 21:27:30,947] Trial 10 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:27:50,584] Trial 11 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:27:56,887] Trial 12 finished with value: 0.36793221728152026 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:28:06,247] Trial 13 finished with value: 0.37557176317823543 and parameters: {'learning_rate': 0.1, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:28:25,821] Trial 14 finished with value: 0.36927342955863734 and parameters: {'learning_rate': 0.04, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:28:32,054] Trial 15 finished with value: 0.36793221728152026 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 9}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:28:41,547] Trial 16 finished with value: 0.3702579273552147 and parameters: {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 7}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:28:46,221] Trial 17 finished with value: 0.36966380525106823 and parameters: {'learning_rate': 0.05, 'depth': 7, 'l2_leaf_reg': 4}. Best is trial 10 with value: 0.3678680030799496.\n",
      "[I 2024-12-05 21:29:05,789] Trial 18 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:29:15,165] Trial 19 finished with value: 0.3679670022475438 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:29:19,821] Trial 20 finished with value: 0.36875794378003324 and parameters: {'learning_rate': 0.05, 'depth': 7, 'l2_leaf_reg': 7}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:29:40,737] Trial 21 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:29:59,903] Trial 22 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:30:09,512] Trial 23 finished with value: 0.3679670022475438 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:30:28,511] Trial 24 finished with value: 0.36798241824514444 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 6}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:30:34,730] Trial 25 finished with value: 0.36859764432806347 and parameters: {'learning_rate': 0.04, 'depth': 8, 'l2_leaf_reg': 7}. Best is trial 18 with value: 0.3675559842359247.\n",
      "[I 2024-12-05 21:30:44,220] Trial 26 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:30:53,692] Trial 27 finished with value: 0.37047843699067806 and parameters: {'learning_rate': 0.06, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:31:03,213] Trial 28 finished with value: 0.36938754781420446 and parameters: {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 5}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:31:22,438] Trial 29 finished with value: 0.3695739069184958 and parameters: {'learning_rate': 0.04, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:31:28,912] Trial 30 finished with value: 0.3715925731975543 and parameters: {'learning_rate': 0.08, 'depth': 8, 'l2_leaf_reg': 6}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:31:48,513] Trial 31 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:32:08,487] Trial 32 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:32:27,833] Trial 33 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:32:37,409] Trial 34 finished with value: 0.3679670022475438 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:32:57,121] Trial 35 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:33:01,147] Trial 36 finished with value: 0.3729953719490797 and parameters: {'learning_rate': 0.05, 'depth': 4, 'l2_leaf_reg': 6}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:33:20,705] Trial 37 finished with value: 0.36916309681456466 and parameters: {'learning_rate': 0.04, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:33:30,247] Trial 38 finished with value: 0.3725917210026757 and parameters: {'learning_rate': 0.08, 'depth': 9, 'l2_leaf_reg': 5}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:33:49,780] Trial 39 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:33:59,471] Trial 40 finished with value: 0.36770010946904863 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:34:18,714] Trial 41 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:34:38,245] Trial 42 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:34:41,538] Trial 43 finished with value: 0.3712518792257757 and parameters: {'learning_rate': 0.03, 'depth': 5, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:35:01,079] Trial 44 finished with value: 0.3701472010421007 and parameters: {'learning_rate': 0.04, 'depth': 10, 'l2_leaf_reg': 3}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:35:20,354] Trial 45 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:35:29,783] Trial 46 finished with value: 0.3752041797190345 and parameters: {'learning_rate': 0.09, 'depth': 9, 'l2_leaf_reg': 6}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:35:48,863] Trial 47 finished with value: 0.3678680030799496 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:36:08,997] Trial 48 finished with value: 0.3695739069184958 and parameters: {'learning_rate': 0.04, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:36:18,556] Trial 49 finished with value: 0.37047843699067806 and parameters: {'learning_rate': 0.06, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:36:25,013] Trial 50 finished with value: 0.36793221728152026 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:36:44,780] Trial 51 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:37:05,637] Trial 52 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:37:25,030] Trial 53 finished with value: 0.36795421747982127 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 1}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:37:34,661] Trial 54 finished with value: 0.36770010946904863 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:37:53,683] Trial 55 finished with value: 0.36916309681456466 and parameters: {'learning_rate': 0.04, 'depth': 10, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:38:03,288] Trial 56 finished with value: 0.37281688747365965 and parameters: {'learning_rate': 0.07, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:38:22,481] Trial 57 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:38:27,262] Trial 58 finished with value: 0.3681850648101912 and parameters: {'learning_rate': 0.03, 'depth': 7, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:38:30,488] Trial 59 finished with value: 0.37031990223404 and parameters: {'learning_rate': 0.04, 'depth': 5, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:38:34,434] Trial 60 finished with value: 0.36903511774157316 and parameters: {'learning_rate': 0.05, 'depth': 6, 'l2_leaf_reg': 4}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:38:53,912] Trial 61 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:39:13,857] Trial 62 finished with value: 0.3675559842359247 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:39:33,904] Trial 63 finished with value: 0.36795513932990853 and parameters: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:39:43,504] Trial 64 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:39:49,983] Trial 65 finished with value: 0.36849122606045037 and parameters: {'learning_rate': 0.04, 'depth': 8, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:39:59,394] Trial 66 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:40:09,072] Trial 67 finished with value: 0.36777320299683086 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:40:15,345] Trial 68 finished with value: 0.36849122606045037 and parameters: {'learning_rate': 0.04, 'depth': 8, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:40:26,277] Trial 69 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:40:35,730] Trial 70 finished with value: 0.3723213826251796 and parameters: {'learning_rate': 0.07, 'depth': 9, 'l2_leaf_reg': 6}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:40:45,524] Trial 71 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:40:54,965] Trial 72 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:01,372] Trial 73 finished with value: 0.3679526973892694 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:11,150] Trial 74 finished with value: 0.36709211391152724 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:20,821] Trial 75 finished with value: 0.3750397925244627 and parameters: {'learning_rate': 0.1, 'depth': 9, 'l2_leaf_reg': 7}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:27,247] Trial 76 finished with value: 0.36853898919409306 and parameters: {'learning_rate': 0.04, 'depth': 8, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:36,788] Trial 77 finished with value: 0.3679945361488538 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 9}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:46,419] Trial 78 finished with value: 0.3679670022475438 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 8}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:41:55,856] Trial 79 finished with value: 0.36731376521076 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 3}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:42:05,342] Trial 80 finished with value: 0.36731376521076 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 3}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:42:14,861] Trial 81 finished with value: 0.36731376521076 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 3}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:42:24,521] Trial 82 finished with value: 0.36731376521076 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 3}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:42:30,805] Trial 83 finished with value: 0.36790407561496874 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 2}. Best is trial 26 with value: 0.36709211391152724.\n",
      "[I 2024-12-05 21:42:40,680] Trial 84 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:42:50,135] Trial 85 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:42:59,671] Trial 86 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:43:09,197] Trial 87 finished with value: 0.36833750659346265 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:43:18,653] Trial 88 finished with value: 0.37529354491420835 and parameters: {'learning_rate': 0.09, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:43:26,322] Trial 89 finished with value: 0.3673875180472852 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 1}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:43:35,897] Trial 90 finished with value: 0.36833750659346265 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:43:45,347] Trial 91 finished with value: 0.3671668301370333 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 1}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:43:54,938] Trial 92 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:04,330] Trial 93 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:13,780] Trial 94 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:20,272] Trial 95 finished with value: 0.36790407561496874 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:30,268] Trial 96 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:40,192] Trial 97 finished with value: 0.36833750659346265 and parameters: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:46,629] Trial 98 finished with value: 0.3673875180472852 and parameters: {'learning_rate': 0.03, 'depth': 8, 'l2_leaf_reg': 1}. Best is trial 84 with value: 0.3669071249275959.\n",
      "[I 2024-12-05 21:44:56,098] Trial 99 finished with value: 0.3669071249275959 and parameters: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}. Best is trial 84 with value: 0.3669071249275959.\n"
     ]
    }
   ],
   "source": [
    "study_cat = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study_cat.optimize(objective_cat, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 모델 학습(데이터 train + val : test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "광주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 7.67s, RMSE: 0.4227, MSE: 0.1787, MAE: 0.3185, R2: 0.8100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./model_/gwangju_rf.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.060000000000000005, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 41.51s, RMSE: 0.4007, MSE: 0.1605, MAE: 0.2944, R2: 0.8293\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./model_/gwangju_gbm.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.04, 'max_depth': 8, 'subsample': 0.65, 'colsample_bytree': 0.7, 'booster': 'gbtree', 'n_estimators': 250}\n",
      "XGBoost - Training Time: 1.58s, RMSE: 0.4064, MSE: 0.1652, MAE: 0.3018, R2: 0.8243\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./model_/gwangju_xgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.02, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000564 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1828\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.267165\n",
      "LightGBM - Training Time: 2.98s, RMSE: 0.4012, MSE: 0.1609, MAE: 0.2960, R2: 0.8289\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./model_/gwangju_lgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 6}\n",
      "CatBoost - Training Time: 9.90s, RMSE: 0.3998, MSE: 0.1599, MAE: 0.2971, R2: 0.8300\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./model_/gwangju_cat.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 7.96s, RMSE: 0.4313, MSE: 0.1861, MAE: 0.3223, R2: 0.8177\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./model_/busan_rf.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 8, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 17.43s, RMSE: 0.4177, MSE: 0.1744, MAE: 0.2997, R2: 0.8291\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./model_/busan_gbm.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.03, 'max_depth': 9, 'subsample': 0.9, 'colsample_bytree': 0.9, 'booster': 'gbtree', 'n_estimators': 250}\n",
      "XGBoost - Training Time: 1.95s, RMSE: 0.4063, MSE: 0.1651, MAE: 0.2980, R2: 0.8383\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./model_/busan_xgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2253\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.253945\n",
      "LightGBM - Training Time: 2.74s, RMSE: 0.4072, MSE: 0.1658, MAE: 0.2976, R2: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./model_/busan_lgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 10, 'l2_leaf_reg': 5}\n",
      "CatBoost - Training Time: 21.93s, RMSE: 0.4072, MSE: 0.1658, MAE: 0.3008, R2: 0.8376\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./model_/busan_cat.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 7.77s, RMSE: 0.3806, MSE: 0.1448, MAE: 0.2757, R2: 0.8341\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./model_/daegu_rf.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.04, 'max_depth': 6, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 51.05s, RMSE: 0.3775, MSE: 0.1425, MAE: 0.2665, R2: 0.8367\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./model_/daegu_gbm.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.02, 'max_depth': 8, 'subsample': 0.65, 'colsample_bytree': 0.8500000000000001, 'booster': 'gbtree', 'n_estimators': 500}\n",
      "XGBoost - Training Time: 2.68s, RMSE: 0.3725, MSE: 0.1388, MAE: 0.2650, R2: 0.8410\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./model_/daegu_xgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.060000000000000005, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1890\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.313424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 16.32s, RMSE: 0.3785, MSE: 0.1433, MAE: 0.2703, R2: 0.8359\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./model_/daegu_lgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 9}\n",
      "CatBoost - Training Time: 10.17s, RMSE: 0.3694, MSE: 0.1364, MAE: 0.2632, R2: 0.8437\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./model_/daegu_cat.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 7.72s, RMSE: 0.4188, MSE: 0.1754, MAE: 0.3085, R2: 0.8297\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./model_/daejeon_rf.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 7, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 62.62s, RMSE: 0.4053, MSE: 0.1643, MAE: 0.2904, R2: 0.8404\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./model_/daejeon_gbm.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 8, 'subsample': 0.7, 'colsample_bytree': 0.8500000000000001, 'booster': 'gbtree', 'n_estimators': 1000}\n",
      "XGBoost - Training Time: 4.62s, RMSE: 0.4005, MSE: 0.1604, MAE: 0.2904, R2: 0.8442\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./model_/daejeon_xgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000892 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1677\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.356206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 2.45s, RMSE: 0.4051, MSE: 0.1641, MAE: 0.2927, R2: 0.8406\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./model_/daejeon_lgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.04, 'depth': 9, 'l2_leaf_reg': 5}\n",
      "CatBoost - Training Time: 9.05s, RMSE: 0.4022, MSE: 0.1618, MAE: 0.2905, R2: 0.8429\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./model_/daejeon_cat.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 7.83s, RMSE: 0.4243, MSE: 0.1801, MAE: 0.3198, R2: 0.7893\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./model_/incheon_rf.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 5, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 40.40s, RMSE: 0.4102, MSE: 0.1682, MAE: 0.2999, R2: 0.8031\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./model_/incheon_gbm.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.02, 'max_depth': 6, 'subsample': 0.5, 'colsample_bytree': 0.8500000000000001, 'booster': 'gbtree', 'n_estimators': 500}\n",
      "XGBoost - Training Time: 2.71s, RMSE: 0.4043, MSE: 0.1634, MAE: 0.2994, R2: 0.8088\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./model_/incheon_xgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.03, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'dart', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1803\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.143254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 15.21s, RMSE: 0.4133, MSE: 0.1708, MAE: 0.3121, R2: 0.8001\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./model_/incheon_lgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 8}\n",
      "CatBoost - Training Time: 9.21s, RMSE: 0.3956, MSE: 0.1565, MAE: 0.2919, R2: 0.8169\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./model_/incheon_cat.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서울"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'n_estimators': 128, 'max_features': 'log2'}\n",
      "Random Forest - Training Time: 7.70s, RMSE: 0.4369, MSE: 0.1909, MAE: 0.3247, R2: 0.7745\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "best_params_rf = study_rf.best_params\n",
    "print(\"Best parameters for Random Forest:\", best_params_rf)\n",
    "\n",
    "# 모델 학습 및 저장\n",
    "start_time = time.time()\n",
    "final_model_rf = RandomForestRegressor(**best_params_rf, random_state=SEED)\n",
    "final_model_rf.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_rf = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_rf, \"./model_/seoul_rf.pkl\")\n",
    "\n",
    "# 모델 성능 평가\n",
    "final_predictions_rf = final_model_rf.predict(X_test_rf_gbm)\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_rf))\n",
    "final_mse_rf = mean_squared_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_mae_rf = mean_absolute_error(y_test_rf_gbm, final_predictions_rf)\n",
    "final_r2_rf = r2_score(y_test_rf_gbm, final_predictions_rf)\n",
    "\n",
    "print(f\"Random Forest - Training Time: {training_time_rf:.2f}s, RMSE: {final_rmse_rf:.4f}, MSE: {final_mse_rf:.4f}, MAE: {final_mae_rf:.4f}, R2: {final_r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Gradient Boosting: {'n_estimators': 500, 'learning_rate': 0.03, 'max_depth': 6, 'loss': 'huber'}\n",
      "Gradient Boosting - Training Time: 50.83s, RMSE: 0.4245, MSE: 0.1802, MAE: 0.3075, R2: 0.7870\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "best_params_gbm = study_gbm.best_params\n",
    "print(\"Best parameters for Gradient Boosting:\", best_params_gbm)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_gbm = GradientBoostingRegressor(**best_params_gbm)\n",
    "final_model_gbm.fit(X_train_val_rf_gbm, y_train_val_rf_gbm)\n",
    "training_time_gbm = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_gbm, \"./model_/seoul_gbm.pkl\")\n",
    "\n",
    "final_predictions_gbm = final_model_gbm.predict(X_test_rf_gbm)\n",
    "final_rmse_gbm = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_gbm))\n",
    "final_mse_gbm = mean_squared_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_mae_gbm = mean_absolute_error(y_test_rf_gbm, final_predictions_gbm)\n",
    "final_r2_gbm = r2_score(y_test_rf_gbm, final_predictions_gbm)\n",
    "print(f\"Gradient Boosting - Training Time: {training_time_gbm:.2f}s, RMSE: {final_rmse_gbm:.4f}, MSE: {final_mse_gbm:.4f}, MAE: {final_mae_gbm:.4f}, R2: {final_r2_gbm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'learning_rate': 0.04, 'max_depth': 7, 'subsample': 0.6, 'colsample_bytree': 1.0, 'booster': 'dart', 'n_estimators': 250}\n",
      "XGBoost - Training Time: 19.26s, RMSE: 0.4222, MSE: 0.1783, MAE: 0.3077, R2: 0.7893\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "best_params_xgb = study_xgb.best_params\n",
    "print(\"Best parameters for XGBoost:\", best_params_xgb)\n",
    "\n",
    "best_num_boost_round = best_params_xgb.pop('n_estimators')\n",
    "start_time = time.time()\n",
    "final_model_xgb = xgb.train(best_params_xgb, dtrain_val, num_boost_round=best_num_boost_round)\n",
    "training_time_xgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_xgb, \"./model_/seoul_xgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_xgb = final_model_xgb.predict(dtest)\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_xgb = np.sqrt(mean_squared_error(dtest.get_label(), final_predictions_xgb))\n",
    "final_mse_xgb = mean_squared_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_mae_xgb = mean_absolute_error(dtest.get_label(), final_predictions_xgb)\n",
    "final_r2_xgb = r2_score(dtest.get_label(), final_predictions_xgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"XGBoost - Training Time: {training_time_xgb:.2f}s, RMSE: {final_rmse_xgb:.4f}, MSE: {final_mse_xgb:.4f}, MAE: {final_mae_xgb:.4f}, R2: {final_r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'num_leaves': 64, 'subsample': 0.5, 'feature_fraction': 1.0, 'boosting_type': 'gbdt', 'n_estimators': 1000}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1897\n",
      "[LightGBM] [Info] Number of data points in the train set: 32142, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 1.115901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM - Training Time: 3.04s, RMSE: 0.4218, MSE: 0.1779, MAE: 0.3062, R2: 0.7897\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "best_params_lgb = study_lgb.best_params\n",
    "print(\"Best parameters for LightGBM:\", best_params_lgb)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_lgb = lgb.train(best_params_lgb, ltrain_val, num_boost_round=best_params_lgb['n_estimators'])\n",
    "training_time_lgb = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_lgb, \"./model_/seoul_lgb.pkl\")\n",
    "\n",
    "# 예측 (test 데이터로 예측)\n",
    "final_predictions_lgb = final_model_lgb.predict(X_test_rf_gbm)  # X_test_rf_gbm 사용\n",
    "\n",
    "# 평가 지표 계산\n",
    "final_rmse_lgb = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_lgb))\n",
    "final_mse_lgb = mean_squared_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_mae_lgb = mean_absolute_error(y_test_rf_gbm, final_predictions_lgb)\n",
    "final_r2_lgb = r2_score(y_test_rf_gbm, final_predictions_lgb)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"LightGBM - Training Time: {training_time_lgb:.2f}s, RMSE: {final_rmse_lgb:.4f}, MSE: {final_mse_lgb:.4f}, MAE: {final_mae_lgb:.4f}, R2: {final_r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for CatBoost: {'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2}\n",
      "CatBoost - Training Time: 10.00s, RMSE: 0.4180, MSE: 0.1747, MAE: 0.3058, R2: 0.7935\n"
     ]
    }
   ],
   "source": [
    "# CatBoost\n",
    "best_params_cat = study_cat.best_params\n",
    "print(\"Best parameters for CatBoost:\", best_params_cat)\n",
    "\n",
    "start_time = time.time()\n",
    "final_model_cat = cb.CatBoostRegressor(**best_params_cat, verbose=0, random_seed=SEED)\n",
    "final_model_cat.fit(ctrain_val)\n",
    "training_time_cat = time.time() - start_time\n",
    "\n",
    "# 학습된 모델 저장\n",
    "joblib.dump(final_model_cat, \"./model_/seoul_cat.pkl\")\n",
    "\n",
    "final_predictions_cat = final_model_cat.predict(ctest)\n",
    "final_rmse_cat = np.sqrt(mean_squared_error(y_test_rf_gbm, final_predictions_cat))\n",
    "final_mse_cat = mean_squared_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_mae_cat = mean_absolute_error(y_test_rf_gbm, final_predictions_cat)\n",
    "final_r2_cat = r2_score(y_test_rf_gbm, final_predictions_cat)\n",
    "print(f\"CatBoost - Training Time: {training_time_cat:.2f}s, RMSE: {final_rmse_cat:.4f}, MSE: {final_mse_cat:.4f}, MAE: {final_mae_cat:.4f}, R2: {final_r2_cat:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
